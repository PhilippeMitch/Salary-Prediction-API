{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3ada487d-223b-4f0c-872e-2c71b135f14d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the necessay libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    GradientBoostingClassifier\n",
    ")\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    fbeta_score,\n",
    "    f1_score,\n",
    "    accuracy_score, \n",
    "    recall_score, \n",
    "    precision_score, \n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix, \n",
    "    classification_report\n",
    ")\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd32465-2f30-4ae0-8ce7-79fc9ccd96f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlgt</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass   fnlgt  education      marital-status  \\\n",
       "0   39         State-gov   77516  Bachelors       Never-married   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors  Married-civ-spouse   \n",
       "2   38           Private  215646    HS-grad            Divorced   \n",
       "\n",
       "          occupation   relationship   race   sex  capital-gain  capital-loss  \\\n",
       "0       Adm-clerical  Not-in-family  White  Male          2174             0   \n",
       "1    Exec-managerial        Husband  White  Male             0             0   \n",
       "2  Handlers-cleaners  Not-in-family  White  Male             0             0   \n",
       "\n",
       "   hours-per-week native-country salary  \n",
       "0              40  United-States  <=50K  \n",
       "1              13  United-States  <=50K  \n",
       "2              40  United-States  <=50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data into a dataframe\n",
    "df = pd.read_csv('data/clean_census.csv')\n",
    "# Display the three first rows from the dataframe\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9bd0b4a-9806-4e4a-b194-c3e5878aaf98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data(\n",
    "    X, categorical_features=[], label=None, training=True, encoder=None, lb=None\n",
    "):\n",
    "    \"\"\" Process the data used in the machine learning pipeline.\n",
    "\n",
    "    Processes the data using one hot encoding for the categorical features and a\n",
    "    label binarizer for the labels. This can be used in either training or\n",
    "    inference/validation.\n",
    "\n",
    "    Note: depending on the type of model used, you may want to add in functionality that\n",
    "    scales the continuous data.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    X : pd.DataFrame\n",
    "        Dataframe containing the features and label. Columns in `categorical_features`\n",
    "    categorical_features: list[str]\n",
    "        List containing the names of the categorical features (default=[])\n",
    "    label : str\n",
    "        Name of the label column in `X`. If None, then an empty array will be returned\n",
    "        for y (default=None)\n",
    "    training : bool\n",
    "        Indicator if training mode or inference/validation mode.\n",
    "    encoder : sklearn.preprocessing._encoders.OneHotEncoder\n",
    "        Trained sklearn OneHotEncoder, only used if training=False.\n",
    "    lb : sklearn.preprocessing._label.LabelBinarizer\n",
    "        Trained sklearn LabelBinarizer, only used if training=False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.array\n",
    "        Processed data.\n",
    "    y : np.array\n",
    "        Processed labels if labeled=True, otherwise empty np.array.\n",
    "    encoder : sklearn.preprocessing._encoders.OneHotEncoder\n",
    "        Trained OneHotEncoder if training is True, otherwise returns the encoder passed\n",
    "        in.\n",
    "    lb : sklearn.preprocessing._label.LabelBinarizer\n",
    "        Trained LabelBinarizer if training is True, otherwise returns the binarizer\n",
    "        passed in.\n",
    "    \"\"\"\n",
    "\n",
    "    if label is not None:\n",
    "        y = X[label]\n",
    "        X = X.drop([label], axis=1)\n",
    "    else:\n",
    "        y = np.array([])\n",
    "\n",
    "    X_categorical = X[categorical_features].values\n",
    "    X_continuous = X.drop(*[categorical_features], axis=1)\n",
    "\n",
    "    if training is True:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "        lb = LabelBinarizer()\n",
    "        X_categorical = encoder.fit_transform(X_categorical)\n",
    "        y = lb.fit_transform(y.values).ravel()\n",
    "    else:\n",
    "        X_categorical = encoder.transform(X_categorical)\n",
    "        try:\n",
    "            y = lb.fit_transform(y.values).ravel()\n",
    "        # Catch the case where y is None because we're doing inference.\n",
    "        except AttributeError as error:\n",
    "            print(\"Error occur: \", error)\n",
    "\n",
    "    X = np.concatenate([X_continuous, X_categorical], axis=1)\n",
    "    return X, y, encoder, lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f7cd9a-e51d-4b9d-9244-12de9f5235ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['workclass',\n",
       " 'education',\n",
       " 'marital-status',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'native-country']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the categorical feature except the column salary\n",
    "categorical_features = list(df.select_dtypes(['object', 'category']).columns)[:-1]\n",
    "# Show the columns\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f74d6e-2621-4666-98fc-f6e1f354ed08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the dataset into train and test\n",
    "train, test = train_test_split(df, shuffle=True, stratify=None, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe76b1c8-f35a-4c0f-809d-e838b888f136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the OneHotEncoder and LabelBinarizer() objects\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "binarizer = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be644ac7-ba9c-4f74-baef-5e698e4a86c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the metrics from the trained model\n",
    "def compute_model_metrics(y, preds):\n",
    "    \"\"\"\n",
    "    Validates the trained machine learning model using precision, recall, and F1.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    y : np.array\n",
    "        Known labels, binarized.\n",
    "    preds : np.array\n",
    "        Predicted labels, binarized.\n",
    "    Returns\n",
    "    -------\n",
    "    precision : float\n",
    "    recall : float\n",
    "    fbeta : float\n",
    "    \"\"\"\n",
    "    fbeta = fbeta_score(y, preds, beta=1, zero_division=1)\n",
    "    precision = precision_score(y, preds, zero_division=1)\n",
    "    recall = recall_score(y, preds, zero_division=1)\n",
    "    return precision, recall, fbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fae87f9-8ebd-437a-a9e0-8f06fc93333e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional: implement hyperparameter tuning.\n",
    "def train_model(X_train, y_train, models):\n",
    "    \"\"\"\n",
    "    Trains a machine learning model and returns it.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    X_train : np.array\n",
    "        Training data.\n",
    "    y_train : np.array\n",
    "        Labels.\n",
    "    Returns\n",
    "    -------\n",
    "    model\n",
    "        Trained machine learning model.\n",
    "    \"\"\"\n",
    "\n",
    "    for key in models.keys():\n",
    "\n",
    "        models[key].fit(X_train, y_train)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b9df36-038d-4618-a290-7defc516835f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the processed train data\n",
    "X_train, y_train, encoder, lb = process_data(\n",
    "                        train, categorical_features=categorical_features, label=\"salary\", training=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70b8eabb-0c8b-47ab-ba22-a22aa567213b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for differents models\n",
    "models = {}\n",
    "models['Logistic Regression'] = LogisticRegression()\n",
    "models['Extrat Classfier'] = ExtraTreesClassifier(n_estimators=50)\n",
    "models['Support Vector Machines'] = LinearSVC()\n",
    "models['Gradient Boosting'] = GradientBoostingClassifier(n_estimators=333, learning_rate=0.8, max_depth=5, random_state=0)\n",
    "models['Decision Trees'] = DecisionTreeClassifier()\n",
    "models['Random Forest'] = RandomForestClassifier()\n",
    "models['XGB Classifier'] = XGBClassifier(objective='binary:logistic', eta=0.3, max_depth= 5, eval_metric = 'aucpr')\n",
    "models['Naive Bayes'] = GaussianNB()\n",
    "models['K-Nearest Neighbor'] = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6062bebe-6196-4c6d-9bd3-dc79d03738cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mitch/miniconda3/envs/ml_model_to_cloud_env/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trained_models = train_model(X_train, y_train, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b1c083b-bd7a-40e3-b925-ae4c816f3676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the result metrics into a dataframe\n",
    "def df_model_results(trained_models, X_data, y_data):\n",
    "    \"\"\"\n",
    "    Get model result into a dataframe\n",
    "    \n",
    "    Inputs:\n",
    "    -------\n",
    "    trained_models : dict\n",
    "                A dictionary of the trained models\n",
    "    X_data : numpy.ndarray\n",
    "        The data we want to evaluate the model on\n",
    "    y_data : numpy.ndarray\n",
    "        The actual label of the data\n",
    "    Returns:\n",
    "    --------\n",
    "    df_model : pandas.core.frame.DataFrame\n",
    "            The evalution results\n",
    "    \"\"\"\n",
    "    fbeta, precision, recall = {}, {}, {}\n",
    "    for key in trained_models.keys():\n",
    "\n",
    "        predictions = trained_models[key].predict(X_data)\n",
    "\n",
    "        fbeta[key] = fbeta_score(y_data, predictions, beta=1, zero_division=1)\n",
    "        precision[key] = precision_score(predictions, y_data)\n",
    "        recall[key] = recall_score(predictions, y_data)\n",
    "\n",
    "    df_model = pd.DataFrame(index=models.keys(), columns=['fbeta', 'precision', 'recall'])\n",
    "    df_model['fbeta'] = fbeta.values()\n",
    "    df_model['precision'] = precision.values()\n",
    "    df_model['recall'] = recall.values()\n",
    "\n",
    "    return df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d458a006-29db-4af4-90e4-75c039d06ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fbeta</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.384543</td>\n",
       "      <td>0.262508</td>\n",
       "      <td>0.718613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extrat Classfier</th>\n",
       "      <td>0.999920</td>\n",
       "      <td>0.999840</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machines</th>\n",
       "      <td>0.214951</td>\n",
       "      <td>0.122194</td>\n",
       "      <td>0.892272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.980250</td>\n",
       "      <td>0.974984</td>\n",
       "      <td>0.985573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Trees</th>\n",
       "      <td>0.999920</td>\n",
       "      <td>0.999840</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.999840</td>\n",
       "      <td>0.999840</td>\n",
       "      <td>0.999840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB Classifier</th>\n",
       "      <td>0.757015</td>\n",
       "      <td>0.698685</td>\n",
       "      <td>0.825972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.418610</td>\n",
       "      <td>0.306607</td>\n",
       "      <td>0.659538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K-Nearest Neighbor</th>\n",
       "      <td>0.564904</td>\n",
       "      <td>0.452213</td>\n",
       "      <td>0.752401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            fbeta  precision    recall\n",
       "Logistic Regression      0.384543   0.262508  0.718613\n",
       "Extrat Classfier         0.999920   0.999840  1.000000\n",
       "Support Vector Machines  0.214951   0.122194  0.892272\n",
       "Gradient Boosting        0.980250   0.974984  0.985573\n",
       "Decision Trees           0.999920   0.999840  1.000000\n",
       "Random Forest            0.999840   0.999840  0.999840\n",
       "XGB Classifier           0.757015   0.698685  0.825972\n",
       "Naive Bayes              0.418610   0.306607  0.659538\n",
       "K-Nearest Neighbor       0.564904   0.452213  0.752401"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the result of the model for the train data into a dataframe\n",
    "df_train_results = df_model_results(trained_models, X_train, y_train)\n",
    "df_train_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6f5b0-c2a9-4283-8392-1101d8b3663c",
   "metadata": {},
   "source": [
    "Extrat Classfier, Decision Trees and Random Forest have the best result on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9535663c-4b95-4f07-8c15-598b455f1f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the test processed data\n",
    "X_test, y_test, encoder, lb = process_data(\n",
    "                        test, categorical_features=categorical_features, \n",
    "                        label=\"salary\", training=False, encoder=encoder, lb=binarizer\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10d27035-c250-4f63-9d4a-d40af39b5d16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fbeta</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.404826</td>\n",
       "      <td>0.282595</td>\n",
       "      <td>0.713386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extrat Classfier</th>\n",
       "      <td>0.649077</td>\n",
       "      <td>0.613849</td>\n",
       "      <td>0.688593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machines</th>\n",
       "      <td>0.253369</td>\n",
       "      <td>0.146600</td>\n",
       "      <td>0.932540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.682452</td>\n",
       "      <td>0.656269</td>\n",
       "      <td>0.710811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Trees</th>\n",
       "      <td>0.630860</td>\n",
       "      <td>0.633812</td>\n",
       "      <td>0.627936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.684318</td>\n",
       "      <td>0.628821</td>\n",
       "      <td>0.750558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB Classifier</th>\n",
       "      <td>0.720922</td>\n",
       "      <td>0.663132</td>\n",
       "      <td>0.789747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.436333</td>\n",
       "      <td>0.328135</td>\n",
       "      <td>0.650990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K-Nearest Neighbor</th>\n",
       "      <td>0.418696</td>\n",
       "      <td>0.332502</td>\n",
       "      <td>0.565217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            fbeta  precision    recall\n",
       "Logistic Regression      0.404826   0.282595  0.713386\n",
       "Extrat Classfier         0.649077   0.613849  0.688593\n",
       "Support Vector Machines  0.253369   0.146600  0.932540\n",
       "Gradient Boosting        0.682452   0.656269  0.710811\n",
       "Decision Trees           0.630860   0.633812  0.627936\n",
       "Random Forest            0.684318   0.628821  0.750558\n",
       "XGB Classifier           0.720922   0.663132  0.789747\n",
       "Naive Bayes              0.436333   0.328135  0.650990\n",
       "K-Nearest Neighbor       0.418696   0.332502  0.565217"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the result metrics for the test data into a dataframe\n",
    "df_test_results = df_model_results(trained_models, X_test, y_test)\n",
    "df_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a00491-2ab5-4260-b121-ce7a6a6687d0",
   "metadata": {},
   "source": [
    "On the test set the XGB classifier has the best result. We will use Baysian Optimisation to get the best hyperparameter to improve the result for XGB Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf46fe-2127-4569-a060-46ab4b80c31f",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d90b1755-7e0a-4a2c-832a-d8b31e6dee3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from bayes_opt import BayesianOptimization\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b0797dd-45a7-4c97-9083-d1f7b0d7b880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bo_tune_xgb(max_depth, gamma, eta):\n",
    "    \"\"\"\n",
    "    Function with the internals we wish to maximize\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    max_depth : tuple\n",
    "            Range of maximum depth of a tree.\n",
    "    gamma : tuple\n",
    "        Range of minimum loss reduction required to make a further \n",
    "        partition on a leaf node of the tree.\n",
    "    eta : tuple\n",
    "        Range of step size shrinkage used in update to prevents overfitting. \n",
    "                \n",
    "    \"\"\"\n",
    "    # Define the value range for the parameters\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'max_depth': int(max_depth),\n",
    "        'eta': eta,\n",
    "        'eval_metric': 'aucpr'\n",
    "    }\n",
    "    \n",
    "    #Cross validating with the specified parameters in 5 folds and 70 iterations\n",
    "    cv_result = xgb.cv(params, training_xgb_matrix, num_boost_round=70, nfold=5)\n",
    "    #Return the resul\n",
    "    cv_result = cv_result['train-aucpr-mean'].iloc[-1]\n",
    "    return 1.0 * cv_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d809977-77bd-4170-b34d-491001c1a92d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate a BayesianOptimization\n",
    "xgb_bo = BayesianOptimization(\n",
    "    bo_tune_xgb, {\n",
    "        'max_depth': (3, 7),\n",
    "        'gamma': (0, 1),\n",
    "        'eta': (0.01, 0.4),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7571b373-3435-4853-ae6f-3d4870975b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group the train data into a xgb.DMatrix\n",
    "training_xgb_matrix = xgb.DMatrix(X_train, label=y_train)\n",
    "test_xgb_matrix = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74980200-16ed-401e-b7cd-676e28118086",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    eta    |   gamma   | max_depth |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8942   \u001b[0m | \u001b[0m0.3361   \u001b[0m | \u001b[0m0.8429   \u001b[0m | \u001b[0m6.482    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.8276   \u001b[0m | \u001b[0m0.2018   \u001b[0m | \u001b[0m0.7963   \u001b[0m | \u001b[0m3.78     \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.7714   \u001b[0m | \u001b[0m0.03079  \u001b[0m | \u001b[0m0.4343   \u001b[0m | \u001b[0m3.8      \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.8411   \u001b[0m | \u001b[0m0.3269   \u001b[0m | \u001b[0m0.986    \u001b[0m | \u001b[0m3.322    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.7874   \u001b[0m | \u001b[0m0.05831  \u001b[0m | \u001b[0m0.5886   \u001b[0m | \u001b[0m3.396    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.863    \u001b[0m | \u001b[0m0.3736   \u001b[0m | \u001b[0m0.7115   \u001b[0m | \u001b[0m4.303    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.8575   \u001b[0m | \u001b[0m0.1159   \u001b[0m | \u001b[0m0.5573   \u001b[0m | \u001b[0m6.26     \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.8031   \u001b[0m | \u001b[0m0.05215  \u001b[0m | \u001b[0m0.9851   \u001b[0m | \u001b[0m4.228    \u001b[0m |\n",
      "| \u001b[95m9        \u001b[0m | \u001b[95m0.9012   \u001b[0m | \u001b[95m0.3885   \u001b[0m | \u001b[95m0.6484   \u001b[0m | \u001b[95m6.747    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.8711   \u001b[0m | \u001b[0m0.1799   \u001b[0m | \u001b[0m0.6937   \u001b[0m | \u001b[0m6.572    \u001b[0m |\n",
      "| \u001b[95m11       \u001b[0m | \u001b[95m0.9016   \u001b[0m | \u001b[95m0.4      \u001b[0m | \u001b[95m0.6681   \u001b[0m | \u001b[95m6.731    \u001b[0m |\n",
      "| \u001b[95m12       \u001b[0m | \u001b[95m0.9219   \u001b[0m | \u001b[95m0.4      \u001b[0m | \u001b[95m0.0      \u001b[0m | \u001b[95m7.0      \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8826   \u001b[0m | \u001b[0m0.4      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m5.26     \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.8826   \u001b[0m | \u001b[0m0.4      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m5.49     \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the the optimization\n",
    "xgb_bo.maximize(n_iter=6, init_points=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d44f1bf-2736-48be-bd2d-ce5f392106e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eta': 0.4, 'gamma': 0.0, 'max_depth': 7.0}\n"
     ]
    }
   ],
   "source": [
    "# Show the best hypermarameters\n",
    "params = xgb_bo.max['params']\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "261b0763-17b8-4624-9a3c-90ddc7e9ed5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-aucpr:0.77350\ttrain-aucpr:0.77043\n",
      "[1]\teval-aucpr:0.79435\ttrain-aucpr:0.79564\n",
      "[2]\teval-aucpr:0.80644\ttrain-aucpr:0.81153\n",
      "[3]\teval-aucpr:0.80829\ttrain-aucpr:0.81502\n",
      "[4]\teval-aucpr:0.81316\ttrain-aucpr:0.82061\n",
      "[5]\teval-aucpr:0.81560\ttrain-aucpr:0.82458\n",
      "[6]\teval-aucpr:0.81805\ttrain-aucpr:0.82832\n",
      "[7]\teval-aucpr:0.81865\ttrain-aucpr:0.83145\n",
      "[8]\teval-aucpr:0.81965\ttrain-aucpr:0.83581\n",
      "[9]\teval-aucpr:0.81998\ttrain-aucpr:0.83716\n",
      "[10]\teval-aucpr:0.82251\ttrain-aucpr:0.83926\n",
      "[11]\teval-aucpr:0.82356\ttrain-aucpr:0.84159\n",
      "[12]\teval-aucpr:0.82467\ttrain-aucpr:0.84316\n",
      "[13]\teval-aucpr:0.82968\ttrain-aucpr:0.85013\n",
      "[14]\teval-aucpr:0.83047\ttrain-aucpr:0.85362\n",
      "[15]\teval-aucpr:0.83015\ttrain-aucpr:0.85416\n",
      "[16]\teval-aucpr:0.83040\ttrain-aucpr:0.85695\n",
      "[17]\teval-aucpr:0.83073\ttrain-aucpr:0.86047\n",
      "[18]\teval-aucpr:0.83360\ttrain-aucpr:0.86259\n",
      "[19]\teval-aucpr:0.83381\ttrain-aucpr:0.86356\n",
      "[20]\teval-aucpr:0.83483\ttrain-aucpr:0.86505\n",
      "[21]\teval-aucpr:0.83494\ttrain-aucpr:0.86594\n",
      "[22]\teval-aucpr:0.83497\ttrain-aucpr:0.86656\n",
      "[23]\teval-aucpr:0.83446\ttrain-aucpr:0.86911\n",
      "[24]\teval-aucpr:0.83393\ttrain-aucpr:0.87169\n",
      "[25]\teval-aucpr:0.83489\ttrain-aucpr:0.87238\n",
      "[26]\teval-aucpr:0.83520\ttrain-aucpr:0.87331\n",
      "[27]\teval-aucpr:0.83528\ttrain-aucpr:0.87471\n",
      "[28]\teval-aucpr:0.83505\ttrain-aucpr:0.87526\n",
      "[29]\teval-aucpr:0.83466\ttrain-aucpr:0.87593\n",
      "[30]\teval-aucpr:0.83453\ttrain-aucpr:0.87738\n",
      "[31]\teval-aucpr:0.83428\ttrain-aucpr:0.87917\n",
      "[32]\teval-aucpr:0.83471\ttrain-aucpr:0.87939\n",
      "[33]\teval-aucpr:0.83515\ttrain-aucpr:0.88038\n",
      "[34]\teval-aucpr:0.83454\ttrain-aucpr:0.88434\n",
      "[35]\teval-aucpr:0.83508\ttrain-aucpr:0.88466\n",
      "[36]\teval-aucpr:0.83541\ttrain-aucpr:0.88630\n",
      "[37]\teval-aucpr:0.83564\ttrain-aucpr:0.88779\n",
      "[38]\teval-aucpr:0.83573\ttrain-aucpr:0.88801\n",
      "[39]\teval-aucpr:0.83601\ttrain-aucpr:0.88809\n",
      "[40]\teval-aucpr:0.83583\ttrain-aucpr:0.88832\n",
      "[41]\teval-aucpr:0.83605\ttrain-aucpr:0.88871\n",
      "[42]\teval-aucpr:0.83562\ttrain-aucpr:0.88917\n",
      "[43]\teval-aucpr:0.83608\ttrain-aucpr:0.88961\n",
      "[44]\teval-aucpr:0.83629\ttrain-aucpr:0.89015\n",
      "[45]\teval-aucpr:0.83662\ttrain-aucpr:0.89026\n",
      "[46]\teval-aucpr:0.83660\ttrain-aucpr:0.89176\n",
      "[47]\teval-aucpr:0.83671\ttrain-aucpr:0.89299\n",
      "[48]\teval-aucpr:0.83652\ttrain-aucpr:0.89380\n",
      "[49]\teval-aucpr:0.83627\ttrain-aucpr:0.89520\n",
      "[50]\teval-aucpr:0.83551\ttrain-aucpr:0.89684\n",
      "[51]\teval-aucpr:0.83553\ttrain-aucpr:0.89722\n",
      "[52]\teval-aucpr:0.83523\ttrain-aucpr:0.89788\n",
      "[53]\teval-aucpr:0.83484\ttrain-aucpr:0.89885\n",
      "[54]\teval-aucpr:0.83458\ttrain-aucpr:0.89962\n",
      "[55]\teval-aucpr:0.83444\ttrain-aucpr:0.90106\n",
      "[56]\teval-aucpr:0.83392\ttrain-aucpr:0.90198\n",
      "[57]\teval-aucpr:0.83407\ttrain-aucpr:0.90205\n",
      "[58]\teval-aucpr:0.83408\ttrain-aucpr:0.90254\n",
      "[59]\teval-aucpr:0.83317\ttrain-aucpr:0.90363\n",
      "[60]\teval-aucpr:0.83285\ttrain-aucpr:0.90448\n",
      "[61]\teval-aucpr:0.83267\ttrain-aucpr:0.90635\n",
      "[62]\teval-aucpr:0.83261\ttrain-aucpr:0.90690\n",
      "[63]\teval-aucpr:0.83237\ttrain-aucpr:0.90851\n",
      "[64]\teval-aucpr:0.83236\ttrain-aucpr:0.90912\n",
      "[65]\teval-aucpr:0.83260\ttrain-aucpr:0.91060\n",
      "[66]\teval-aucpr:0.83233\ttrain-aucpr:0.91103\n",
      "[67]\teval-aucpr:0.83244\ttrain-aucpr:0.91105\n",
      "[68]\teval-aucpr:0.83234\ttrain-aucpr:0.91130\n",
      "[69]\teval-aucpr:0.83239\ttrain-aucpr:0.91174\n",
      "[70]\teval-aucpr:0.83226\ttrain-aucpr:0.91232\n",
      "[71]\teval-aucpr:0.83250\ttrain-aucpr:0.91263\n",
      "[72]\teval-aucpr:0.83276\ttrain-aucpr:0.91338\n",
      "[73]\teval-aucpr:0.83157\ttrain-aucpr:0.91543\n",
      "[74]\teval-aucpr:0.83177\ttrain-aucpr:0.91602\n",
      "[75]\teval-aucpr:0.83173\ttrain-aucpr:0.91735\n",
      "[76]\teval-aucpr:0.83168\ttrain-aucpr:0.91895\n",
      "[77]\teval-aucpr:0.83191\ttrain-aucpr:0.91946\n",
      "[78]\teval-aucpr:0.83178\ttrain-aucpr:0.91979\n",
      "[79]\teval-aucpr:0.83192\ttrain-aucpr:0.91982\n",
      "[80]\teval-aucpr:0.83175\ttrain-aucpr:0.91999\n",
      "[81]\teval-aucpr:0.83064\ttrain-aucpr:0.92153\n",
      "[82]\teval-aucpr:0.83025\ttrain-aucpr:0.92289\n",
      "[83]\teval-aucpr:0.83042\ttrain-aucpr:0.92400\n",
      "[84]\teval-aucpr:0.83058\ttrain-aucpr:0.92584\n",
      "[85]\teval-aucpr:0.83046\ttrain-aucpr:0.92607\n",
      "[86]\teval-aucpr:0.83063\ttrain-aucpr:0.92649\n",
      "[87]\teval-aucpr:0.83038\ttrain-aucpr:0.92802\n",
      "[88]\teval-aucpr:0.83021\ttrain-aucpr:0.92820\n",
      "[89]\teval-aucpr:0.83006\ttrain-aucpr:0.93002\n",
      "[90]\teval-aucpr:0.82990\ttrain-aucpr:0.93042\n",
      "[91]\teval-aucpr:0.82998\ttrain-aucpr:0.93089\n",
      "[92]\teval-aucpr:0.83022\ttrain-aucpr:0.93136\n",
      "[93]\teval-aucpr:0.83023\ttrain-aucpr:0.93143\n",
      "[94]\teval-aucpr:0.83009\ttrain-aucpr:0.93164\n",
      "[95]\teval-aucpr:0.83011\ttrain-aucpr:0.93175\n",
      "[96]\teval-aucpr:0.83003\ttrain-aucpr:0.93251\n",
      "[97]\teval-aucpr:0.83009\ttrain-aucpr:0.93270\n",
      "[98]\teval-aucpr:0.83030\ttrain-aucpr:0.93381\n",
      "[99]\teval-aucpr:0.83058\ttrain-aucpr:0.93488\n",
      "[100]\teval-aucpr:0.83043\ttrain-aucpr:0.93529\n",
      "[101]\teval-aucpr:0.83044\ttrain-aucpr:0.93555\n",
      "[102]\teval-aucpr:0.83045\ttrain-aucpr:0.93594\n",
      "[103]\teval-aucpr:0.83005\ttrain-aucpr:0.93655\n",
      "[104]\teval-aucpr:0.83033\ttrain-aucpr:0.93759\n",
      "[105]\teval-aucpr:0.82988\ttrain-aucpr:0.93883\n",
      "[106]\teval-aucpr:0.82983\ttrain-aucpr:0.93907\n",
      "[107]\teval-aucpr:0.82965\ttrain-aucpr:0.93934\n",
      "[108]\teval-aucpr:0.82946\ttrain-aucpr:0.93997\n",
      "[109]\teval-aucpr:0.82955\ttrain-aucpr:0.94096\n",
      "[110]\teval-aucpr:0.82925\ttrain-aucpr:0.94158\n",
      "[111]\teval-aucpr:0.82896\ttrain-aucpr:0.94211\n",
      "[112]\teval-aucpr:0.82886\ttrain-aucpr:0.94270\n",
      "[113]\teval-aucpr:0.82852\ttrain-aucpr:0.94337\n",
      "[114]\teval-aucpr:0.82841\ttrain-aucpr:0.94375\n",
      "[115]\teval-aucpr:0.82841\ttrain-aucpr:0.94381\n",
      "[116]\teval-aucpr:0.82833\ttrain-aucpr:0.94386\n",
      "[117]\teval-aucpr:0.82827\ttrain-aucpr:0.94403\n",
      "[118]\teval-aucpr:0.82823\ttrain-aucpr:0.94425\n",
      "[119]\teval-aucpr:0.82745\ttrain-aucpr:0.94554\n",
      "[120]\teval-aucpr:0.82724\ttrain-aucpr:0.94671\n",
      "[121]\teval-aucpr:0.82738\ttrain-aucpr:0.94672\n",
      "[122]\teval-aucpr:0.82752\ttrain-aucpr:0.94686\n",
      "[123]\teval-aucpr:0.82776\ttrain-aucpr:0.94751\n",
      "[124]\teval-aucpr:0.82778\ttrain-aucpr:0.94779\n",
      "[125]\teval-aucpr:0.82785\ttrain-aucpr:0.94794\n",
      "[126]\teval-aucpr:0.82775\ttrain-aucpr:0.94831\n",
      "[127]\teval-aucpr:0.82772\ttrain-aucpr:0.94862\n",
      "[128]\teval-aucpr:0.82750\ttrain-aucpr:0.94881\n",
      "[129]\teval-aucpr:0.82764\ttrain-aucpr:0.94904\n",
      "[130]\teval-aucpr:0.82747\ttrain-aucpr:0.94924\n",
      "[131]\teval-aucpr:0.82739\ttrain-aucpr:0.94981\n",
      "[132]\teval-aucpr:0.82743\ttrain-aucpr:0.95008\n",
      "[133]\teval-aucpr:0.82711\ttrain-aucpr:0.95041\n",
      "[134]\teval-aucpr:0.82702\ttrain-aucpr:0.95079\n",
      "[135]\teval-aucpr:0.82702\ttrain-aucpr:0.95094\n",
      "[136]\teval-aucpr:0.82710\ttrain-aucpr:0.95099\n",
      "[137]\teval-aucpr:0.82728\ttrain-aucpr:0.95152\n",
      "[138]\teval-aucpr:0.82749\ttrain-aucpr:0.95162\n",
      "[139]\teval-aucpr:0.82738\ttrain-aucpr:0.95213\n",
      "[140]\teval-aucpr:0.82651\ttrain-aucpr:0.95325\n",
      "[141]\teval-aucpr:0.82644\ttrain-aucpr:0.95355\n",
      "[142]\teval-aucpr:0.82651\ttrain-aucpr:0.95366\n",
      "[143]\teval-aucpr:0.82617\ttrain-aucpr:0.95463\n",
      "[144]\teval-aucpr:0.82601\ttrain-aucpr:0.95497\n",
      "[145]\teval-aucpr:0.82547\ttrain-aucpr:0.95542\n",
      "[146]\teval-aucpr:0.82544\ttrain-aucpr:0.95570\n",
      "[147]\teval-aucpr:0.82540\ttrain-aucpr:0.95573\n",
      "[148]\teval-aucpr:0.82524\ttrain-aucpr:0.95600\n",
      "[149]\teval-aucpr:0.82516\ttrain-aucpr:0.95666\n",
      "[150]\teval-aucpr:0.82524\ttrain-aucpr:0.95721\n",
      "[151]\teval-aucpr:0.82426\ttrain-aucpr:0.95813\n",
      "[152]\teval-aucpr:0.82398\ttrain-aucpr:0.95869\n",
      "[153]\teval-aucpr:0.82393\ttrain-aucpr:0.95899\n",
      "[154]\teval-aucpr:0.82413\ttrain-aucpr:0.95920\n",
      "[155]\teval-aucpr:0.82408\ttrain-aucpr:0.95923\n",
      "[156]\teval-aucpr:0.82425\ttrain-aucpr:0.95960\n",
      "[157]\teval-aucpr:0.82378\ttrain-aucpr:0.96115\n",
      "[158]\teval-aucpr:0.82369\ttrain-aucpr:0.96148\n",
      "[159]\teval-aucpr:0.82369\ttrain-aucpr:0.96192\n",
      "[160]\teval-aucpr:0.82309\ttrain-aucpr:0.96348\n",
      "[161]\teval-aucpr:0.82297\ttrain-aucpr:0.96397\n",
      "[162]\teval-aucpr:0.82305\ttrain-aucpr:0.96401\n",
      "[163]\teval-aucpr:0.82304\ttrain-aucpr:0.96436\n",
      "[164]\teval-aucpr:0.82301\ttrain-aucpr:0.96443\n",
      "[165]\teval-aucpr:0.82283\ttrain-aucpr:0.96460\n",
      "[166]\teval-aucpr:0.82280\ttrain-aucpr:0.96478\n",
      "[167]\teval-aucpr:0.82277\ttrain-aucpr:0.96480\n",
      "[168]\teval-aucpr:0.82284\ttrain-aucpr:0.96492\n",
      "[169]\teval-aucpr:0.82270\ttrain-aucpr:0.96504\n",
      "[170]\teval-aucpr:0.82255\ttrain-aucpr:0.96552\n",
      "[171]\teval-aucpr:0.82251\ttrain-aucpr:0.96575\n",
      "[172]\teval-aucpr:0.82232\ttrain-aucpr:0.96608\n",
      "[173]\teval-aucpr:0.82230\ttrain-aucpr:0.96667\n",
      "[174]\teval-aucpr:0.82224\ttrain-aucpr:0.96677\n",
      "[175]\teval-aucpr:0.82189\ttrain-aucpr:0.96777\n",
      "[176]\teval-aucpr:0.82142\ttrain-aucpr:0.96824\n",
      "[177]\teval-aucpr:0.82147\ttrain-aucpr:0.96866\n",
      "[178]\teval-aucpr:0.82092\ttrain-aucpr:0.96954\n",
      "[179]\teval-aucpr:0.82100\ttrain-aucpr:0.96958\n",
      "[180]\teval-aucpr:0.82127\ttrain-aucpr:0.96972\n",
      "[181]\teval-aucpr:0.82135\ttrain-aucpr:0.96981\n",
      "[182]\teval-aucpr:0.82135\ttrain-aucpr:0.97011\n",
      "[183]\teval-aucpr:0.82132\ttrain-aucpr:0.97035\n",
      "[184]\teval-aucpr:0.82094\ttrain-aucpr:0.97101\n",
      "[185]\teval-aucpr:0.82106\ttrain-aucpr:0.97120\n",
      "[186]\teval-aucpr:0.82035\ttrain-aucpr:0.97201\n",
      "[187]\teval-aucpr:0.82017\ttrain-aucpr:0.97266\n",
      "[188]\teval-aucpr:0.82003\ttrain-aucpr:0.97281\n",
      "[189]\teval-aucpr:0.82024\ttrain-aucpr:0.97289\n",
      "[190]\teval-aucpr:0.82015\ttrain-aucpr:0.97312\n",
      "[191]\teval-aucpr:0.81985\ttrain-aucpr:0.97350\n",
      "[192]\teval-aucpr:0.81948\ttrain-aucpr:0.97433\n",
      "[193]\teval-aucpr:0.81966\ttrain-aucpr:0.97453\n",
      "[194]\teval-aucpr:0.81951\ttrain-aucpr:0.97490\n",
      "[195]\teval-aucpr:0.81949\ttrain-aucpr:0.97536\n",
      "[196]\teval-aucpr:0.81945\ttrain-aucpr:0.97585\n",
      "[197]\teval-aucpr:0.81899\ttrain-aucpr:0.97628\n",
      "[198]\teval-aucpr:0.81897\ttrain-aucpr:0.97669\n",
      "[199]\teval-aucpr:0.81898\ttrain-aucpr:0.97684\n",
      "[200]\teval-aucpr:0.81906\ttrain-aucpr:0.97687\n",
      "[201]\teval-aucpr:0.81891\ttrain-aucpr:0.97702\n",
      "[202]\teval-aucpr:0.81878\ttrain-aucpr:0.97750\n",
      "[203]\teval-aucpr:0.81862\ttrain-aucpr:0.97768\n",
      "[204]\teval-aucpr:0.81886\ttrain-aucpr:0.97784\n",
      "[205]\teval-aucpr:0.81867\ttrain-aucpr:0.97822\n",
      "[206]\teval-aucpr:0.81879\ttrain-aucpr:0.97822\n",
      "[207]\teval-aucpr:0.81878\ttrain-aucpr:0.97879\n",
      "[208]\teval-aucpr:0.81842\ttrain-aucpr:0.97901\n",
      "[209]\teval-aucpr:0.81854\ttrain-aucpr:0.97941\n",
      "[210]\teval-aucpr:0.81840\ttrain-aucpr:0.97943\n",
      "[211]\teval-aucpr:0.81835\ttrain-aucpr:0.97946\n",
      "[212]\teval-aucpr:0.81847\ttrain-aucpr:0.97946\n",
      "[213]\teval-aucpr:0.81842\ttrain-aucpr:0.97948\n",
      "[214]\teval-aucpr:0.81832\ttrain-aucpr:0.97967\n",
      "[215]\teval-aucpr:0.81807\ttrain-aucpr:0.97983\n",
      "[216]\teval-aucpr:0.81803\ttrain-aucpr:0.97985\n",
      "[217]\teval-aucpr:0.81800\ttrain-aucpr:0.97999\n",
      "[218]\teval-aucpr:0.81779\ttrain-aucpr:0.98043\n",
      "[219]\teval-aucpr:0.81785\ttrain-aucpr:0.98062\n",
      "[220]\teval-aucpr:0.81782\ttrain-aucpr:0.98069\n",
      "[221]\teval-aucpr:0.81767\ttrain-aucpr:0.98127\n",
      "[222]\teval-aucpr:0.81757\ttrain-aucpr:0.98140\n",
      "[223]\teval-aucpr:0.81735\ttrain-aucpr:0.98154\n",
      "[224]\teval-aucpr:0.81688\ttrain-aucpr:0.98187\n",
      "[225]\teval-aucpr:0.81690\ttrain-aucpr:0.98200\n",
      "[226]\teval-aucpr:0.81687\ttrain-aucpr:0.98225\n",
      "[227]\teval-aucpr:0.81651\ttrain-aucpr:0.98273\n",
      "[228]\teval-aucpr:0.81645\ttrain-aucpr:0.98288\n",
      "[229]\teval-aucpr:0.81629\ttrain-aucpr:0.98290\n",
      "[230]\teval-aucpr:0.81618\ttrain-aucpr:0.98297\n",
      "[231]\teval-aucpr:0.81613\ttrain-aucpr:0.98300\n",
      "[232]\teval-aucpr:0.81594\ttrain-aucpr:0.98315\n",
      "[233]\teval-aucpr:0.81602\ttrain-aucpr:0.98315\n",
      "[234]\teval-aucpr:0.81578\ttrain-aucpr:0.98363\n",
      "[235]\teval-aucpr:0.81564\ttrain-aucpr:0.98379\n",
      "[236]\teval-aucpr:0.81571\ttrain-aucpr:0.98395\n",
      "[237]\teval-aucpr:0.81572\ttrain-aucpr:0.98406\n",
      "[238]\teval-aucpr:0.81533\ttrain-aucpr:0.98417\n",
      "[239]\teval-aucpr:0.81529\ttrain-aucpr:0.98422\n",
      "[240]\teval-aucpr:0.81507\ttrain-aucpr:0.98427\n",
      "[241]\teval-aucpr:0.81495\ttrain-aucpr:0.98428\n",
      "[242]\teval-aucpr:0.81517\ttrain-aucpr:0.98442\n",
      "[243]\teval-aucpr:0.81513\ttrain-aucpr:0.98468\n",
      "[244]\teval-aucpr:0.81505\ttrain-aucpr:0.98477\n",
      "[245]\teval-aucpr:0.81520\ttrain-aucpr:0.98493\n",
      "[246]\teval-aucpr:0.81528\ttrain-aucpr:0.98508\n",
      "[247]\teval-aucpr:0.81525\ttrain-aucpr:0.98514\n",
      "[248]\teval-aucpr:0.81501\ttrain-aucpr:0.98519\n",
      "[249]\teval-aucpr:0.81505\ttrain-aucpr:0.98519\n",
      "[250]\teval-aucpr:0.81509\ttrain-aucpr:0.98520\n",
      "[251]\teval-aucpr:0.81509\ttrain-aucpr:0.98531\n",
      "[252]\teval-aucpr:0.81504\ttrain-aucpr:0.98531\n",
      "[253]\teval-aucpr:0.81504\ttrain-aucpr:0.98539\n",
      "[254]\teval-aucpr:0.81495\ttrain-aucpr:0.98547\n",
      "[255]\teval-aucpr:0.81490\ttrain-aucpr:0.98556\n",
      "[256]\teval-aucpr:0.81489\ttrain-aucpr:0.98562\n",
      "[257]\teval-aucpr:0.81488\ttrain-aucpr:0.98585\n",
      "[258]\teval-aucpr:0.81470\ttrain-aucpr:0.98594\n",
      "[259]\teval-aucpr:0.81457\ttrain-aucpr:0.98606\n",
      "[260]\teval-aucpr:0.81457\ttrain-aucpr:0.98617\n",
      "[261]\teval-aucpr:0.81420\ttrain-aucpr:0.98659\n",
      "[262]\teval-aucpr:0.81407\ttrain-aucpr:0.98661\n",
      "[263]\teval-aucpr:0.81401\ttrain-aucpr:0.98704\n",
      "[264]\teval-aucpr:0.81393\ttrain-aucpr:0.98705\n",
      "[265]\teval-aucpr:0.81388\ttrain-aucpr:0.98735\n",
      "[266]\teval-aucpr:0.81398\ttrain-aucpr:0.98744\n",
      "[267]\teval-aucpr:0.81401\ttrain-aucpr:0.98749\n",
      "[268]\teval-aucpr:0.81395\ttrain-aucpr:0.98769\n",
      "[269]\teval-aucpr:0.81391\ttrain-aucpr:0.98794\n",
      "[270]\teval-aucpr:0.81386\ttrain-aucpr:0.98804\n",
      "[271]\teval-aucpr:0.81380\ttrain-aucpr:0.98804\n",
      "[272]\teval-aucpr:0.81376\ttrain-aucpr:0.98804\n",
      "[273]\teval-aucpr:0.81371\ttrain-aucpr:0.98806\n",
      "[274]\teval-aucpr:0.81353\ttrain-aucpr:0.98827\n",
      "[275]\teval-aucpr:0.81369\ttrain-aucpr:0.98828\n",
      "[276]\teval-aucpr:0.81366\ttrain-aucpr:0.98835\n",
      "[277]\teval-aucpr:0.81346\ttrain-aucpr:0.98844\n",
      "[278]\teval-aucpr:0.81351\ttrain-aucpr:0.98852\n",
      "[279]\teval-aucpr:0.81335\ttrain-aucpr:0.98869\n",
      "[280]\teval-aucpr:0.81339\ttrain-aucpr:0.98876\n",
      "[281]\teval-aucpr:0.81321\ttrain-aucpr:0.98886\n",
      "[282]\teval-aucpr:0.81329\ttrain-aucpr:0.98887\n",
      "[283]\teval-aucpr:0.81345\ttrain-aucpr:0.98902\n",
      "[284]\teval-aucpr:0.81297\ttrain-aucpr:0.98921\n",
      "[285]\teval-aucpr:0.81291\ttrain-aucpr:0.98938\n",
      "[286]\teval-aucpr:0.81272\ttrain-aucpr:0.98950\n",
      "[287]\teval-aucpr:0.81283\ttrain-aucpr:0.98963\n",
      "[288]\teval-aucpr:0.81257\ttrain-aucpr:0.98991\n",
      "[289]\teval-aucpr:0.81245\ttrain-aucpr:0.99001\n",
      "[290]\teval-aucpr:0.81221\ttrain-aucpr:0.99012\n",
      "[291]\teval-aucpr:0.81211\ttrain-aucpr:0.99015\n",
      "[292]\teval-aucpr:0.81203\ttrain-aucpr:0.99022\n",
      "[293]\teval-aucpr:0.81186\ttrain-aucpr:0.99048\n",
      "[294]\teval-aucpr:0.81172\ttrain-aucpr:0.99056\n",
      "[295]\teval-aucpr:0.81148\ttrain-aucpr:0.99059\n",
      "[296]\teval-aucpr:0.81138\ttrain-aucpr:0.99078\n",
      "[297]\teval-aucpr:0.81137\ttrain-aucpr:0.99081\n",
      "[298]\teval-aucpr:0.81147\ttrain-aucpr:0.99090\n",
      "[299]\teval-aucpr:0.81117\ttrain-aucpr:0.99097\n",
      "[300]\teval-aucpr:0.81103\ttrain-aucpr:0.99120\n",
      "[301]\teval-aucpr:0.81105\ttrain-aucpr:0.99122\n",
      "[302]\teval-aucpr:0.81089\ttrain-aucpr:0.99127\n",
      "[303]\teval-aucpr:0.81055\ttrain-aucpr:0.99140\n",
      "[304]\teval-aucpr:0.81039\ttrain-aucpr:0.99163\n",
      "[305]\teval-aucpr:0.81067\ttrain-aucpr:0.99184\n",
      "[306]\teval-aucpr:0.81076\ttrain-aucpr:0.99206\n",
      "[307]\teval-aucpr:0.81072\ttrain-aucpr:0.99214\n",
      "[308]\teval-aucpr:0.81058\ttrain-aucpr:0.99216\n",
      "[309]\teval-aucpr:0.81031\ttrain-aucpr:0.99226\n",
      "[310]\teval-aucpr:0.81017\ttrain-aucpr:0.99231\n",
      "[311]\teval-aucpr:0.81002\ttrain-aucpr:0.99244\n",
      "[312]\teval-aucpr:0.80995\ttrain-aucpr:0.99252\n",
      "[313]\teval-aucpr:0.81023\ttrain-aucpr:0.99267\n",
      "[314]\teval-aucpr:0.81010\ttrain-aucpr:0.99280\n",
      "[315]\teval-aucpr:0.81000\ttrain-aucpr:0.99284\n",
      "[316]\teval-aucpr:0.80990\ttrain-aucpr:0.99287\n",
      "[317]\teval-aucpr:0.80993\ttrain-aucpr:0.99287\n",
      "[318]\teval-aucpr:0.80978\ttrain-aucpr:0.99291\n",
      "[319]\teval-aucpr:0.80968\ttrain-aucpr:0.99292\n",
      "[320]\teval-aucpr:0.80967\ttrain-aucpr:0.99292\n",
      "[321]\teval-aucpr:0.80947\ttrain-aucpr:0.99297\n",
      "[322]\teval-aucpr:0.80957\ttrain-aucpr:0.99301\n",
      "[323]\teval-aucpr:0.80952\ttrain-aucpr:0.99301\n",
      "[324]\teval-aucpr:0.80949\ttrain-aucpr:0.99301\n",
      "[325]\teval-aucpr:0.80956\ttrain-aucpr:0.99304\n",
      "[326]\teval-aucpr:0.80952\ttrain-aucpr:0.99306\n",
      "[327]\teval-aucpr:0.80955\ttrain-aucpr:0.99318\n",
      "[328]\teval-aucpr:0.80952\ttrain-aucpr:0.99320\n",
      "[329]\teval-aucpr:0.80953\ttrain-aucpr:0.99327\n",
      "[330]\teval-aucpr:0.80941\ttrain-aucpr:0.99337\n",
      "[331]\teval-aucpr:0.80932\ttrain-aucpr:0.99343\n",
      "[332]\teval-aucpr:0.80909\ttrain-aucpr:0.99355\n",
      "[333]\teval-aucpr:0.80901\ttrain-aucpr:0.99361\n",
      "[334]\teval-aucpr:0.80896\ttrain-aucpr:0.99364\n",
      "[335]\teval-aucpr:0.80897\ttrain-aucpr:0.99367\n",
      "[336]\teval-aucpr:0.80894\ttrain-aucpr:0.99369\n",
      "[337]\teval-aucpr:0.80893\ttrain-aucpr:0.99373\n",
      "[338]\teval-aucpr:0.80894\ttrain-aucpr:0.99377\n",
      "[339]\teval-aucpr:0.80909\ttrain-aucpr:0.99378\n",
      "[340]\teval-aucpr:0.80920\ttrain-aucpr:0.99381\n",
      "[341]\teval-aucpr:0.80920\ttrain-aucpr:0.99387\n",
      "[342]\teval-aucpr:0.80938\ttrain-aucpr:0.99396\n",
      "[343]\teval-aucpr:0.80927\ttrain-aucpr:0.99407\n",
      "[344]\teval-aucpr:0.80928\ttrain-aucpr:0.99408\n",
      "[345]\teval-aucpr:0.80908\ttrain-aucpr:0.99416\n",
      "[346]\teval-aucpr:0.80909\ttrain-aucpr:0.99416\n",
      "[347]\teval-aucpr:0.80900\ttrain-aucpr:0.99418\n",
      "[348]\teval-aucpr:0.80879\ttrain-aucpr:0.99426\n",
      "[349]\teval-aucpr:0.80858\ttrain-aucpr:0.99430\n",
      "[350]\teval-aucpr:0.80862\ttrain-aucpr:0.99435\n",
      "[351]\teval-aucpr:0.80804\ttrain-aucpr:0.99448\n",
      "[352]\teval-aucpr:0.80785\ttrain-aucpr:0.99451\n",
      "[353]\teval-aucpr:0.80784\ttrain-aucpr:0.99456\n",
      "[354]\teval-aucpr:0.80779\ttrain-aucpr:0.99465\n",
      "[355]\teval-aucpr:0.80770\ttrain-aucpr:0.99470\n",
      "[356]\teval-aucpr:0.80756\ttrain-aucpr:0.99481\n",
      "[357]\teval-aucpr:0.80753\ttrain-aucpr:0.99481\n",
      "[358]\teval-aucpr:0.80742\ttrain-aucpr:0.99482\n",
      "[359]\teval-aucpr:0.80748\ttrain-aucpr:0.99492\n",
      "[360]\teval-aucpr:0.80758\ttrain-aucpr:0.99493\n",
      "[361]\teval-aucpr:0.80763\ttrain-aucpr:0.99495\n",
      "[362]\teval-aucpr:0.80765\ttrain-aucpr:0.99495\n",
      "[363]\teval-aucpr:0.80768\ttrain-aucpr:0.99495\n",
      "[364]\teval-aucpr:0.80772\ttrain-aucpr:0.99497\n",
      "[365]\teval-aucpr:0.80758\ttrain-aucpr:0.99500\n",
      "[366]\teval-aucpr:0.80748\ttrain-aucpr:0.99505\n",
      "[367]\teval-aucpr:0.80741\ttrain-aucpr:0.99513\n",
      "[368]\teval-aucpr:0.80727\ttrain-aucpr:0.99525\n",
      "[369]\teval-aucpr:0.80717\ttrain-aucpr:0.99527\n",
      "[370]\teval-aucpr:0.80719\ttrain-aucpr:0.99535\n",
      "[371]\teval-aucpr:0.80702\ttrain-aucpr:0.99540\n",
      "[372]\teval-aucpr:0.80704\ttrain-aucpr:0.99541\n",
      "[373]\teval-aucpr:0.80707\ttrain-aucpr:0.99541\n",
      "[374]\teval-aucpr:0.80689\ttrain-aucpr:0.99554\n",
      "[375]\teval-aucpr:0.80671\ttrain-aucpr:0.99565\n",
      "[376]\teval-aucpr:0.80675\ttrain-aucpr:0.99567\n",
      "[377]\teval-aucpr:0.80654\ttrain-aucpr:0.99579\n",
      "[378]\teval-aucpr:0.80646\ttrain-aucpr:0.99582\n",
      "[379]\teval-aucpr:0.80648\ttrain-aucpr:0.99585\n",
      "[380]\teval-aucpr:0.80652\ttrain-aucpr:0.99588\n",
      "[381]\teval-aucpr:0.80662\ttrain-aucpr:0.99590\n",
      "[382]\teval-aucpr:0.80649\ttrain-aucpr:0.99590\n",
      "[383]\teval-aucpr:0.80642\ttrain-aucpr:0.99589\n",
      "[384]\teval-aucpr:0.80646\ttrain-aucpr:0.99591\n",
      "[385]\teval-aucpr:0.80637\ttrain-aucpr:0.99602\n",
      "[386]\teval-aucpr:0.80623\ttrain-aucpr:0.99606\n",
      "[387]\teval-aucpr:0.80615\ttrain-aucpr:0.99613\n",
      "[388]\teval-aucpr:0.80611\ttrain-aucpr:0.99616\n",
      "[389]\teval-aucpr:0.80609\ttrain-aucpr:0.99619\n",
      "[390]\teval-aucpr:0.80605\ttrain-aucpr:0.99625\n",
      "[391]\teval-aucpr:0.80614\ttrain-aucpr:0.99640\n",
      "[392]\teval-aucpr:0.80621\ttrain-aucpr:0.99642\n",
      "[393]\teval-aucpr:0.80614\ttrain-aucpr:0.99645\n",
      "[394]\teval-aucpr:0.80617\ttrain-aucpr:0.99648\n",
      "[395]\teval-aucpr:0.80628\ttrain-aucpr:0.99651\n",
      "[396]\teval-aucpr:0.80639\ttrain-aucpr:0.99653\n",
      "[397]\teval-aucpr:0.80643\ttrain-aucpr:0.99656\n",
      "[398]\teval-aucpr:0.80649\ttrain-aucpr:0.99660\n",
      "[399]\teval-aucpr:0.80647\ttrain-aucpr:0.99666\n",
      "[400]\teval-aucpr:0.80629\ttrain-aucpr:0.99672\n",
      "[401]\teval-aucpr:0.80631\ttrain-aucpr:0.99680\n",
      "[402]\teval-aucpr:0.80622\ttrain-aucpr:0.99697\n",
      "[403]\teval-aucpr:0.80609\ttrain-aucpr:0.99699\n",
      "[404]\teval-aucpr:0.80608\ttrain-aucpr:0.99704\n",
      "[405]\teval-aucpr:0.80601\ttrain-aucpr:0.99706\n",
      "[406]\teval-aucpr:0.80583\ttrain-aucpr:0.99710\n",
      "[407]\teval-aucpr:0.80576\ttrain-aucpr:0.99712\n",
      "[408]\teval-aucpr:0.80575\ttrain-aucpr:0.99713\n",
      "[409]\teval-aucpr:0.80567\ttrain-aucpr:0.99714\n",
      "[410]\teval-aucpr:0.80572\ttrain-aucpr:0.99714\n",
      "[411]\teval-aucpr:0.80572\ttrain-aucpr:0.99717\n",
      "[412]\teval-aucpr:0.80563\ttrain-aucpr:0.99718\n",
      "[413]\teval-aucpr:0.80575\ttrain-aucpr:0.99722\n",
      "[414]\teval-aucpr:0.80589\ttrain-aucpr:0.99722\n",
      "[415]\teval-aucpr:0.80595\ttrain-aucpr:0.99722\n",
      "[416]\teval-aucpr:0.80591\ttrain-aucpr:0.99724\n",
      "[417]\teval-aucpr:0.80563\ttrain-aucpr:0.99728\n",
      "[418]\teval-aucpr:0.80560\ttrain-aucpr:0.99729\n",
      "[419]\teval-aucpr:0.80547\ttrain-aucpr:0.99730\n",
      "[420]\teval-aucpr:0.80551\ttrain-aucpr:0.99731\n",
      "[421]\teval-aucpr:0.80545\ttrain-aucpr:0.99732\n",
      "[422]\teval-aucpr:0.80537\ttrain-aucpr:0.99733\n",
      "[423]\teval-aucpr:0.80526\ttrain-aucpr:0.99735\n",
      "[424]\teval-aucpr:0.80520\ttrain-aucpr:0.99735\n",
      "[425]\teval-aucpr:0.80499\ttrain-aucpr:0.99736\n",
      "[426]\teval-aucpr:0.80502\ttrain-aucpr:0.99737\n",
      "[427]\teval-aucpr:0.80489\ttrain-aucpr:0.99741\n",
      "[428]\teval-aucpr:0.80483\ttrain-aucpr:0.99744\n",
      "[429]\teval-aucpr:0.80480\ttrain-aucpr:0.99744\n",
      "[430]\teval-aucpr:0.80479\ttrain-aucpr:0.99748\n",
      "[431]\teval-aucpr:0.80519\ttrain-aucpr:0.99755\n",
      "[432]\teval-aucpr:0.80524\ttrain-aucpr:0.99766\n",
      "[433]\teval-aucpr:0.80521\ttrain-aucpr:0.99768\n",
      "[434]\teval-aucpr:0.80527\ttrain-aucpr:0.99777\n",
      "[435]\teval-aucpr:0.80522\ttrain-aucpr:0.99785\n",
      "[436]\teval-aucpr:0.80532\ttrain-aucpr:0.99785\n",
      "[437]\teval-aucpr:0.80532\ttrain-aucpr:0.99786\n",
      "[438]\teval-aucpr:0.80533\ttrain-aucpr:0.99787\n",
      "[439]\teval-aucpr:0.80536\ttrain-aucpr:0.99787\n",
      "[440]\teval-aucpr:0.80545\ttrain-aucpr:0.99789\n",
      "[441]\teval-aucpr:0.80529\ttrain-aucpr:0.99794\n",
      "[442]\teval-aucpr:0.80532\ttrain-aucpr:0.99795\n",
      "[443]\teval-aucpr:0.80532\ttrain-aucpr:0.99795\n",
      "[444]\teval-aucpr:0.80528\ttrain-aucpr:0.99800\n",
      "[445]\teval-aucpr:0.80512\ttrain-aucpr:0.99806\n",
      "[446]\teval-aucpr:0.80503\ttrain-aucpr:0.99811\n",
      "[447]\teval-aucpr:0.80495\ttrain-aucpr:0.99813\n",
      "[448]\teval-aucpr:0.80500\ttrain-aucpr:0.99816\n",
      "[449]\teval-aucpr:0.80502\ttrain-aucpr:0.99816\n",
      "[450]\teval-aucpr:0.80495\ttrain-aucpr:0.99818\n",
      "[451]\teval-aucpr:0.80491\ttrain-aucpr:0.99818\n",
      "[452]\teval-aucpr:0.80486\ttrain-aucpr:0.99819\n",
      "[453]\teval-aucpr:0.80441\ttrain-aucpr:0.99829\n",
      "[454]\teval-aucpr:0.80443\ttrain-aucpr:0.99832\n",
      "[455]\teval-aucpr:0.80432\ttrain-aucpr:0.99833\n",
      "[456]\teval-aucpr:0.80436\ttrain-aucpr:0.99832\n",
      "[457]\teval-aucpr:0.80433\ttrain-aucpr:0.99833\n",
      "[458]\teval-aucpr:0.80428\ttrain-aucpr:0.99833\n",
      "[459]\teval-aucpr:0.80435\ttrain-aucpr:0.99833\n",
      "[460]\teval-aucpr:0.80430\ttrain-aucpr:0.99836\n",
      "[461]\teval-aucpr:0.80442\ttrain-aucpr:0.99838\n",
      "[462]\teval-aucpr:0.80426\ttrain-aucpr:0.99840\n",
      "[463]\teval-aucpr:0.80402\ttrain-aucpr:0.99847\n",
      "[464]\teval-aucpr:0.80396\ttrain-aucpr:0.99850\n",
      "[465]\teval-aucpr:0.80377\ttrain-aucpr:0.99853\n",
      "[466]\teval-aucpr:0.80387\ttrain-aucpr:0.99855\n",
      "[467]\teval-aucpr:0.80365\ttrain-aucpr:0.99859\n",
      "[468]\teval-aucpr:0.80378\ttrain-aucpr:0.99859\n",
      "[469]\teval-aucpr:0.80362\ttrain-aucpr:0.99859\n",
      "[470]\teval-aucpr:0.80365\ttrain-aucpr:0.99864\n",
      "[471]\teval-aucpr:0.80334\ttrain-aucpr:0.99868\n",
      "[472]\teval-aucpr:0.80311\ttrain-aucpr:0.99869\n",
      "[473]\teval-aucpr:0.80302\ttrain-aucpr:0.99871\n",
      "[474]\teval-aucpr:0.80288\ttrain-aucpr:0.99873\n",
      "[475]\teval-aucpr:0.80285\ttrain-aucpr:0.99875\n",
      "[476]\teval-aucpr:0.80267\ttrain-aucpr:0.99874\n",
      "[477]\teval-aucpr:0.80258\ttrain-aucpr:0.99874\n",
      "[478]\teval-aucpr:0.80253\ttrain-aucpr:0.99876\n",
      "[479]\teval-aucpr:0.80245\ttrain-aucpr:0.99879\n",
      "[480]\teval-aucpr:0.80246\ttrain-aucpr:0.99880\n",
      "[481]\teval-aucpr:0.80227\ttrain-aucpr:0.99881\n",
      "[482]\teval-aucpr:0.80209\ttrain-aucpr:0.99884\n",
      "[483]\teval-aucpr:0.80210\ttrain-aucpr:0.99884\n",
      "[484]\teval-aucpr:0.80205\ttrain-aucpr:0.99884\n",
      "[485]\teval-aucpr:0.80201\ttrain-aucpr:0.99884\n",
      "[486]\teval-aucpr:0.80207\ttrain-aucpr:0.99884\n",
      "[487]\teval-aucpr:0.80195\ttrain-aucpr:0.99886\n",
      "[488]\teval-aucpr:0.80195\ttrain-aucpr:0.99886\n",
      "[489]\teval-aucpr:0.80190\ttrain-aucpr:0.99886\n",
      "[490]\teval-aucpr:0.80191\ttrain-aucpr:0.99886\n",
      "[491]\teval-aucpr:0.80200\ttrain-aucpr:0.99887\n",
      "[492]\teval-aucpr:0.80192\ttrain-aucpr:0.99887\n",
      "[493]\teval-aucpr:0.80176\ttrain-aucpr:0.99890\n",
      "[494]\teval-aucpr:0.80168\ttrain-aucpr:0.99890\n",
      "[495]\teval-aucpr:0.80164\ttrain-aucpr:0.99893\n",
      "[496]\teval-aucpr:0.80178\ttrain-aucpr:0.99895\n",
      "[497]\teval-aucpr:0.80176\ttrain-aucpr:0.99896\n",
      "[498]\teval-aucpr:0.80170\ttrain-aucpr:0.99896\n",
      "[499]\teval-aucpr:0.80162\ttrain-aucpr:0.99899\n",
      "[500]\teval-aucpr:0.80163\ttrain-aucpr:0.99900\n",
      "[501]\teval-aucpr:0.80168\ttrain-aucpr:0.99900\n",
      "[502]\teval-aucpr:0.80166\ttrain-aucpr:0.99902\n",
      "[503]\teval-aucpr:0.80156\ttrain-aucpr:0.99903\n",
      "[504]\teval-aucpr:0.80156\ttrain-aucpr:0.99903\n",
      "[505]\teval-aucpr:0.80149\ttrain-aucpr:0.99903\n",
      "[506]\teval-aucpr:0.80145\ttrain-aucpr:0.99903\n",
      "[507]\teval-aucpr:0.80148\ttrain-aucpr:0.99904\n",
      "[508]\teval-aucpr:0.80143\ttrain-aucpr:0.99905\n",
      "[509]\teval-aucpr:0.80142\ttrain-aucpr:0.99905\n",
      "[510]\teval-aucpr:0.80147\ttrain-aucpr:0.99905\n",
      "[511]\teval-aucpr:0.80139\ttrain-aucpr:0.99905\n",
      "[512]\teval-aucpr:0.80143\ttrain-aucpr:0.99907\n",
      "[513]\teval-aucpr:0.80141\ttrain-aucpr:0.99907\n",
      "[514]\teval-aucpr:0.80143\ttrain-aucpr:0.99908\n",
      "[515]\teval-aucpr:0.80147\ttrain-aucpr:0.99909\n",
      "[516]\teval-aucpr:0.80149\ttrain-aucpr:0.99909\n",
      "[517]\teval-aucpr:0.80140\ttrain-aucpr:0.99910\n",
      "[518]\teval-aucpr:0.80137\ttrain-aucpr:0.99910\n",
      "[519]\teval-aucpr:0.80119\ttrain-aucpr:0.99911\n",
      "[520]\teval-aucpr:0.80107\ttrain-aucpr:0.99911\n",
      "[521]\teval-aucpr:0.80101\ttrain-aucpr:0.99913\n",
      "[522]\teval-aucpr:0.80106\ttrain-aucpr:0.99914\n",
      "[523]\teval-aucpr:0.80101\ttrain-aucpr:0.99914\n",
      "[524]\teval-aucpr:0.80097\ttrain-aucpr:0.99914\n",
      "[525]\teval-aucpr:0.80084\ttrain-aucpr:0.99915\n",
      "[526]\teval-aucpr:0.80075\ttrain-aucpr:0.99915\n",
      "[527]\teval-aucpr:0.80078\ttrain-aucpr:0.99915\n",
      "[528]\teval-aucpr:0.80081\ttrain-aucpr:0.99915\n",
      "[529]\teval-aucpr:0.80077\ttrain-aucpr:0.99915\n",
      "[530]\teval-aucpr:0.80077\ttrain-aucpr:0.99915\n",
      "[531]\teval-aucpr:0.80070\ttrain-aucpr:0.99915\n",
      "[532]\teval-aucpr:0.80065\ttrain-aucpr:0.99919\n",
      "[533]\teval-aucpr:0.80038\ttrain-aucpr:0.99921\n",
      "[534]\teval-aucpr:0.80039\ttrain-aucpr:0.99921\n",
      "[535]\teval-aucpr:0.80041\ttrain-aucpr:0.99921\n",
      "[536]\teval-aucpr:0.80027\ttrain-aucpr:0.99921\n",
      "[537]\teval-aucpr:0.80015\ttrain-aucpr:0.99922\n",
      "[538]\teval-aucpr:0.80020\ttrain-aucpr:0.99923\n",
      "[539]\teval-aucpr:0.80012\ttrain-aucpr:0.99924\n",
      "[540]\teval-aucpr:0.79998\ttrain-aucpr:0.99926\n",
      "[541]\teval-aucpr:0.79993\ttrain-aucpr:0.99926\n",
      "[542]\teval-aucpr:0.79984\ttrain-aucpr:0.99927\n",
      "[543]\teval-aucpr:0.79976\ttrain-aucpr:0.99927\n",
      "[544]\teval-aucpr:0.79961\ttrain-aucpr:0.99929\n",
      "[545]\teval-aucpr:0.79975\ttrain-aucpr:0.99931\n",
      "[546]\teval-aucpr:0.79954\ttrain-aucpr:0.99934\n",
      "[547]\teval-aucpr:0.79947\ttrain-aucpr:0.99935\n",
      "[548]\teval-aucpr:0.79942\ttrain-aucpr:0.99935\n",
      "[549]\teval-aucpr:0.79936\ttrain-aucpr:0.99936\n",
      "[550]\teval-aucpr:0.79936\ttrain-aucpr:0.99937\n",
      "[551]\teval-aucpr:0.79938\ttrain-aucpr:0.99937\n",
      "[552]\teval-aucpr:0.79931\ttrain-aucpr:0.99939\n",
      "[553]\teval-aucpr:0.79931\ttrain-aucpr:0.99939\n",
      "[554]\teval-aucpr:0.79934\ttrain-aucpr:0.99939\n",
      "[555]\teval-aucpr:0.79927\ttrain-aucpr:0.99940\n",
      "[556]\teval-aucpr:0.79929\ttrain-aucpr:0.99940\n",
      "[557]\teval-aucpr:0.79934\ttrain-aucpr:0.99940\n",
      "[558]\teval-aucpr:0.79937\ttrain-aucpr:0.99940\n",
      "[559]\teval-aucpr:0.79937\ttrain-aucpr:0.99941\n",
      "[560]\teval-aucpr:0.79913\ttrain-aucpr:0.99941\n",
      "[561]\teval-aucpr:0.79916\ttrain-aucpr:0.99941\n",
      "[562]\teval-aucpr:0.79917\ttrain-aucpr:0.99941\n",
      "[563]\teval-aucpr:0.79918\ttrain-aucpr:0.99942\n",
      "[564]\teval-aucpr:0.79883\ttrain-aucpr:0.99945\n",
      "[565]\teval-aucpr:0.79893\ttrain-aucpr:0.99949\n",
      "[566]\teval-aucpr:0.79878\ttrain-aucpr:0.99950\n",
      "[567]\teval-aucpr:0.79865\ttrain-aucpr:0.99950\n",
      "[568]\teval-aucpr:0.79853\ttrain-aucpr:0.99951\n",
      "[569]\teval-aucpr:0.79851\ttrain-aucpr:0.99951\n",
      "[570]\teval-aucpr:0.79850\ttrain-aucpr:0.99951\n",
      "[571]\teval-aucpr:0.79855\ttrain-aucpr:0.99952\n",
      "[572]\teval-aucpr:0.79857\ttrain-aucpr:0.99952\n",
      "[573]\teval-aucpr:0.79860\ttrain-aucpr:0.99953\n",
      "[574]\teval-aucpr:0.79869\ttrain-aucpr:0.99953\n",
      "[575]\teval-aucpr:0.79856\ttrain-aucpr:0.99953\n",
      "[576]\teval-aucpr:0.79850\ttrain-aucpr:0.99954\n",
      "[577]\teval-aucpr:0.79854\ttrain-aucpr:0.99954\n",
      "[578]\teval-aucpr:0.79831\ttrain-aucpr:0.99955\n",
      "[579]\teval-aucpr:0.79840\ttrain-aucpr:0.99956\n",
      "[580]\teval-aucpr:0.79827\ttrain-aucpr:0.99956\n",
      "[581]\teval-aucpr:0.79827\ttrain-aucpr:0.99956\n",
      "[582]\teval-aucpr:0.79823\ttrain-aucpr:0.99956\n",
      "[583]\teval-aucpr:0.79830\ttrain-aucpr:0.99956\n",
      "[584]\teval-aucpr:0.79830\ttrain-aucpr:0.99957\n",
      "[585]\teval-aucpr:0.79827\ttrain-aucpr:0.99957\n",
      "[586]\teval-aucpr:0.79823\ttrain-aucpr:0.99957\n",
      "[587]\teval-aucpr:0.79832\ttrain-aucpr:0.99957\n",
      "[588]\teval-aucpr:0.79836\ttrain-aucpr:0.99958\n",
      "[589]\teval-aucpr:0.79827\ttrain-aucpr:0.99960\n",
      "[590]\teval-aucpr:0.79820\ttrain-aucpr:0.99963\n",
      "[591]\teval-aucpr:0.79806\ttrain-aucpr:0.99964\n",
      "[592]\teval-aucpr:0.79800\ttrain-aucpr:0.99965\n",
      "[593]\teval-aucpr:0.79791\ttrain-aucpr:0.99965\n",
      "[594]\teval-aucpr:0.79784\ttrain-aucpr:0.99966\n",
      "[595]\teval-aucpr:0.79777\ttrain-aucpr:0.99967\n",
      "[596]\teval-aucpr:0.79793\ttrain-aucpr:0.99968\n",
      "[597]\teval-aucpr:0.79795\ttrain-aucpr:0.99968\n",
      "[598]\teval-aucpr:0.79794\ttrain-aucpr:0.99969\n",
      "[599]\teval-aucpr:0.79795\ttrain-aucpr:0.99969\n",
      "[600]\teval-aucpr:0.79792\ttrain-aucpr:0.99969\n",
      "[601]\teval-aucpr:0.79792\ttrain-aucpr:0.99969\n",
      "[602]\teval-aucpr:0.79776\ttrain-aucpr:0.99970\n",
      "[603]\teval-aucpr:0.79783\ttrain-aucpr:0.99971\n",
      "[604]\teval-aucpr:0.79764\ttrain-aucpr:0.99971\n",
      "[605]\teval-aucpr:0.79772\ttrain-aucpr:0.99972\n",
      "[606]\teval-aucpr:0.79768\ttrain-aucpr:0.99972\n",
      "[607]\teval-aucpr:0.79765\ttrain-aucpr:0.99972\n",
      "[608]\teval-aucpr:0.79769\ttrain-aucpr:0.99973\n",
      "[609]\teval-aucpr:0.79756\ttrain-aucpr:0.99973\n",
      "[610]\teval-aucpr:0.79762\ttrain-aucpr:0.99973\n",
      "[611]\teval-aucpr:0.79761\ttrain-aucpr:0.99973\n",
      "[612]\teval-aucpr:0.79766\ttrain-aucpr:0.99974\n",
      "[613]\teval-aucpr:0.79756\ttrain-aucpr:0.99974\n",
      "[614]\teval-aucpr:0.79756\ttrain-aucpr:0.99975\n",
      "[615]\teval-aucpr:0.79761\ttrain-aucpr:0.99975\n",
      "[616]\teval-aucpr:0.79771\ttrain-aucpr:0.99975\n",
      "[617]\teval-aucpr:0.79775\ttrain-aucpr:0.99975\n",
      "[618]\teval-aucpr:0.79756\ttrain-aucpr:0.99976\n",
      "[619]\teval-aucpr:0.79745\ttrain-aucpr:0.99976\n",
      "[620]\teval-aucpr:0.79735\ttrain-aucpr:0.99977\n",
      "[621]\teval-aucpr:0.79731\ttrain-aucpr:0.99977\n",
      "[622]\teval-aucpr:0.79720\ttrain-aucpr:0.99978\n",
      "[623]\teval-aucpr:0.79720\ttrain-aucpr:0.99979\n",
      "[624]\teval-aucpr:0.79718\ttrain-aucpr:0.99979\n",
      "[625]\teval-aucpr:0.79716\ttrain-aucpr:0.99979\n",
      "[626]\teval-aucpr:0.79704\ttrain-aucpr:0.99979\n",
      "[627]\teval-aucpr:0.79711\ttrain-aucpr:0.99979\n",
      "[628]\teval-aucpr:0.79700\ttrain-aucpr:0.99979\n",
      "[629]\teval-aucpr:0.79695\ttrain-aucpr:0.99979\n",
      "[630]\teval-aucpr:0.79679\ttrain-aucpr:0.99979\n",
      "[631]\teval-aucpr:0.79680\ttrain-aucpr:0.99980\n",
      "[632]\teval-aucpr:0.79678\ttrain-aucpr:0.99980\n",
      "[633]\teval-aucpr:0.79672\ttrain-aucpr:0.99980\n",
      "[634]\teval-aucpr:0.79669\ttrain-aucpr:0.99980\n",
      "[635]\teval-aucpr:0.79682\ttrain-aucpr:0.99980\n",
      "[636]\teval-aucpr:0.79678\ttrain-aucpr:0.99981\n",
      "[637]\teval-aucpr:0.79673\ttrain-aucpr:0.99982\n",
      "[638]\teval-aucpr:0.79679\ttrain-aucpr:0.99982\n",
      "[639]\teval-aucpr:0.79681\ttrain-aucpr:0.99982\n",
      "[640]\teval-aucpr:0.79676\ttrain-aucpr:0.99982\n",
      "[641]\teval-aucpr:0.79678\ttrain-aucpr:0.99982\n",
      "[642]\teval-aucpr:0.79683\ttrain-aucpr:0.99982\n",
      "[643]\teval-aucpr:0.79686\ttrain-aucpr:0.99982\n",
      "[644]\teval-aucpr:0.79689\ttrain-aucpr:0.99982\n",
      "[645]\teval-aucpr:0.79681\ttrain-aucpr:0.99982\n",
      "[646]\teval-aucpr:0.79690\ttrain-aucpr:0.99982\n",
      "[647]\teval-aucpr:0.79695\ttrain-aucpr:0.99982\n",
      "[648]\teval-aucpr:0.79703\ttrain-aucpr:0.99983\n",
      "[649]\teval-aucpr:0.79704\ttrain-aucpr:0.99984\n",
      "[650]\teval-aucpr:0.79699\ttrain-aucpr:0.99984\n",
      "[651]\teval-aucpr:0.79698\ttrain-aucpr:0.99984\n",
      "[652]\teval-aucpr:0.79704\ttrain-aucpr:0.99984\n",
      "[653]\teval-aucpr:0.79705\ttrain-aucpr:0.99984\n",
      "[654]\teval-aucpr:0.79712\ttrain-aucpr:0.99984\n",
      "[655]\teval-aucpr:0.79708\ttrain-aucpr:0.99984\n",
      "[656]\teval-aucpr:0.79695\ttrain-aucpr:0.99985\n",
      "[657]\teval-aucpr:0.79682\ttrain-aucpr:0.99985\n",
      "[658]\teval-aucpr:0.79666\ttrain-aucpr:0.99985\n",
      "[659]\teval-aucpr:0.79664\ttrain-aucpr:0.99985\n",
      "[660]\teval-aucpr:0.79668\ttrain-aucpr:0.99985\n",
      "[661]\teval-aucpr:0.79672\ttrain-aucpr:0.99985\n",
      "[662]\teval-aucpr:0.79672\ttrain-aucpr:0.99985\n",
      "[663]\teval-aucpr:0.79680\ttrain-aucpr:0.99985\n",
      "[664]\teval-aucpr:0.79679\ttrain-aucpr:0.99985\n",
      "[665]\teval-aucpr:0.79654\ttrain-aucpr:0.99986\n",
      "[666]\teval-aucpr:0.79651\ttrain-aucpr:0.99986\n",
      "[667]\teval-aucpr:0.79658\ttrain-aucpr:0.99986\n",
      "[668]\teval-aucpr:0.79659\ttrain-aucpr:0.99986\n",
      "[669]\teval-aucpr:0.79659\ttrain-aucpr:0.99986\n",
      "[670]\teval-aucpr:0.79664\ttrain-aucpr:0.99986\n",
      "[671]\teval-aucpr:0.79660\ttrain-aucpr:0.99986\n",
      "[672]\teval-aucpr:0.79659\ttrain-aucpr:0.99986\n",
      "[673]\teval-aucpr:0.79657\ttrain-aucpr:0.99987\n",
      "[674]\teval-aucpr:0.79659\ttrain-aucpr:0.99987\n",
      "[675]\teval-aucpr:0.79655\ttrain-aucpr:0.99988\n",
      "[676]\teval-aucpr:0.79656\ttrain-aucpr:0.99988\n",
      "[677]\teval-aucpr:0.79656\ttrain-aucpr:0.99988\n",
      "[678]\teval-aucpr:0.79659\ttrain-aucpr:0.99988\n",
      "[679]\teval-aucpr:0.79660\ttrain-aucpr:0.99988\n",
      "[680]\teval-aucpr:0.79657\ttrain-aucpr:0.99988\n",
      "[681]\teval-aucpr:0.79643\ttrain-aucpr:0.99988\n",
      "[682]\teval-aucpr:0.79649\ttrain-aucpr:0.99988\n",
      "[683]\teval-aucpr:0.79651\ttrain-aucpr:0.99988\n",
      "[684]\teval-aucpr:0.79652\ttrain-aucpr:0.99988\n",
      "[685]\teval-aucpr:0.79647\ttrain-aucpr:0.99988\n",
      "[686]\teval-aucpr:0.79648\ttrain-aucpr:0.99988\n",
      "[687]\teval-aucpr:0.79646\ttrain-aucpr:0.99988\n",
      "[688]\teval-aucpr:0.79634\ttrain-aucpr:0.99989\n",
      "[689]\teval-aucpr:0.79638\ttrain-aucpr:0.99989\n",
      "[690]\teval-aucpr:0.79642\ttrain-aucpr:0.99989\n",
      "[691]\teval-aucpr:0.79641\ttrain-aucpr:0.99989\n",
      "[692]\teval-aucpr:0.79644\ttrain-aucpr:0.99989\n",
      "[693]\teval-aucpr:0.79642\ttrain-aucpr:0.99989\n",
      "[694]\teval-aucpr:0.79647\ttrain-aucpr:0.99989\n",
      "[695]\teval-aucpr:0.79646\ttrain-aucpr:0.99989\n",
      "[696]\teval-aucpr:0.79645\ttrain-aucpr:0.99989\n",
      "[697]\teval-aucpr:0.79620\ttrain-aucpr:0.99990\n",
      "[698]\teval-aucpr:0.79591\ttrain-aucpr:0.99990\n",
      "[699]\teval-aucpr:0.79579\ttrain-aucpr:0.99990\n",
      "[700]\teval-aucpr:0.79567\ttrain-aucpr:0.99990\n",
      "[701]\teval-aucpr:0.79572\ttrain-aucpr:0.99991\n",
      "[702]\teval-aucpr:0.79575\ttrain-aucpr:0.99991\n",
      "[703]\teval-aucpr:0.79575\ttrain-aucpr:0.99991\n",
      "[704]\teval-aucpr:0.79586\ttrain-aucpr:0.99991\n",
      "[705]\teval-aucpr:0.79589\ttrain-aucpr:0.99991\n",
      "[706]\teval-aucpr:0.79599\ttrain-aucpr:0.99991\n",
      "[707]\teval-aucpr:0.79589\ttrain-aucpr:0.99992\n",
      "[708]\teval-aucpr:0.79591\ttrain-aucpr:0.99992\n",
      "[709]\teval-aucpr:0.79583\ttrain-aucpr:0.99992\n",
      "[710]\teval-aucpr:0.79581\ttrain-aucpr:0.99992\n",
      "[711]\teval-aucpr:0.79578\ttrain-aucpr:0.99992\n",
      "[712]\teval-aucpr:0.79576\ttrain-aucpr:0.99992\n",
      "[713]\teval-aucpr:0.79575\ttrain-aucpr:0.99992\n",
      "[714]\teval-aucpr:0.79579\ttrain-aucpr:0.99992\n",
      "[715]\teval-aucpr:0.79565\ttrain-aucpr:0.99992\n",
      "[716]\teval-aucpr:0.79554\ttrain-aucpr:0.99993\n",
      "[717]\teval-aucpr:0.79553\ttrain-aucpr:0.99993\n",
      "[718]\teval-aucpr:0.79546\ttrain-aucpr:0.99993\n",
      "[719]\teval-aucpr:0.79553\ttrain-aucpr:0.99993\n",
      "[720]\teval-aucpr:0.79547\ttrain-aucpr:0.99993\n",
      "[721]\teval-aucpr:0.79544\ttrain-aucpr:0.99993\n",
      "[722]\teval-aucpr:0.79550\ttrain-aucpr:0.99993\n",
      "[723]\teval-aucpr:0.79547\ttrain-aucpr:0.99993\n",
      "[724]\teval-aucpr:0.79561\ttrain-aucpr:0.99993\n",
      "[725]\teval-aucpr:0.79546\ttrain-aucpr:0.99993\n",
      "[726]\teval-aucpr:0.79560\ttrain-aucpr:0.99993\n",
      "[727]\teval-aucpr:0.79549\ttrain-aucpr:0.99993\n",
      "[728]\teval-aucpr:0.79548\ttrain-aucpr:0.99993\n",
      "[729]\teval-aucpr:0.79544\ttrain-aucpr:0.99993\n",
      "[730]\teval-aucpr:0.79541\ttrain-aucpr:0.99993\n",
      "[731]\teval-aucpr:0.79535\ttrain-aucpr:0.99994\n",
      "[732]\teval-aucpr:0.79508\ttrain-aucpr:0.99994\n",
      "[733]\teval-aucpr:0.79509\ttrain-aucpr:0.99994\n",
      "[734]\teval-aucpr:0.79515\ttrain-aucpr:0.99995\n",
      "[735]\teval-aucpr:0.79504\ttrain-aucpr:0.99995\n",
      "[736]\teval-aucpr:0.79504\ttrain-aucpr:0.99995\n",
      "[737]\teval-aucpr:0.79504\ttrain-aucpr:0.99995\n",
      "[738]\teval-aucpr:0.79510\ttrain-aucpr:0.99995\n",
      "[739]\teval-aucpr:0.79500\ttrain-aucpr:0.99995\n",
      "[740]\teval-aucpr:0.79498\ttrain-aucpr:0.99995\n",
      "[741]\teval-aucpr:0.79504\ttrain-aucpr:0.99995\n",
      "[742]\teval-aucpr:0.79498\ttrain-aucpr:0.99996\n",
      "[743]\teval-aucpr:0.79507\ttrain-aucpr:0.99996\n",
      "[744]\teval-aucpr:0.79490\ttrain-aucpr:0.99996\n",
      "[745]\teval-aucpr:0.79486\ttrain-aucpr:0.99996\n",
      "[746]\teval-aucpr:0.79493\ttrain-aucpr:0.99996\n",
      "[747]\teval-aucpr:0.79484\ttrain-aucpr:0.99996\n",
      "[748]\teval-aucpr:0.79479\ttrain-aucpr:0.99996\n",
      "[749]\teval-aucpr:0.79479\ttrain-aucpr:0.99996\n",
      "[750]\teval-aucpr:0.79476\ttrain-aucpr:0.99996\n",
      "[751]\teval-aucpr:0.79484\ttrain-aucpr:0.99996\n",
      "[752]\teval-aucpr:0.79478\ttrain-aucpr:0.99996\n",
      "[753]\teval-aucpr:0.79478\ttrain-aucpr:0.99996\n",
      "[754]\teval-aucpr:0.79464\ttrain-aucpr:0.99996\n",
      "[755]\teval-aucpr:0.79453\ttrain-aucpr:0.99996\n",
      "[756]\teval-aucpr:0.79458\ttrain-aucpr:0.99996\n",
      "[757]\teval-aucpr:0.79473\ttrain-aucpr:0.99996\n",
      "[758]\teval-aucpr:0.79469\ttrain-aucpr:0.99996\n",
      "[759]\teval-aucpr:0.79463\ttrain-aucpr:0.99996\n",
      "[760]\teval-aucpr:0.79463\ttrain-aucpr:0.99996\n",
      "[761]\teval-aucpr:0.79469\ttrain-aucpr:0.99996\n",
      "[762]\teval-aucpr:0.79478\ttrain-aucpr:0.99997\n",
      "[763]\teval-aucpr:0.79483\ttrain-aucpr:0.99997\n",
      "[764]\teval-aucpr:0.79476\ttrain-aucpr:0.99997\n",
      "[765]\teval-aucpr:0.79471\ttrain-aucpr:0.99997\n",
      "[766]\teval-aucpr:0.79475\ttrain-aucpr:0.99997\n",
      "[767]\teval-aucpr:0.79461\ttrain-aucpr:0.99997\n",
      "[768]\teval-aucpr:0.79470\ttrain-aucpr:0.99997\n",
      "[769]\teval-aucpr:0.79470\ttrain-aucpr:0.99997\n",
      "[770]\teval-aucpr:0.79471\ttrain-aucpr:0.99997\n",
      "[771]\teval-aucpr:0.79427\ttrain-aucpr:0.99997\n",
      "[772]\teval-aucpr:0.79440\ttrain-aucpr:0.99997\n",
      "[773]\teval-aucpr:0.79444\ttrain-aucpr:0.99997\n",
      "[774]\teval-aucpr:0.79443\ttrain-aucpr:0.99997\n",
      "[775]\teval-aucpr:0.79421\ttrain-aucpr:0.99997\n",
      "[776]\teval-aucpr:0.79427\ttrain-aucpr:0.99997\n",
      "[777]\teval-aucpr:0.79430\ttrain-aucpr:0.99997\n",
      "[778]\teval-aucpr:0.79436\ttrain-aucpr:0.99997\n",
      "[779]\teval-aucpr:0.79421\ttrain-aucpr:0.99998\n",
      "[780]\teval-aucpr:0.79407\ttrain-aucpr:0.99998\n",
      "[781]\teval-aucpr:0.79400\ttrain-aucpr:0.99998\n",
      "[782]\teval-aucpr:0.79403\ttrain-aucpr:0.99998\n",
      "[783]\teval-aucpr:0.79392\ttrain-aucpr:0.99998\n",
      "[784]\teval-aucpr:0.79389\ttrain-aucpr:0.99998\n",
      "[785]\teval-aucpr:0.79393\ttrain-aucpr:0.99998\n",
      "[786]\teval-aucpr:0.79389\ttrain-aucpr:0.99998\n",
      "[787]\teval-aucpr:0.79389\ttrain-aucpr:0.99998\n",
      "[788]\teval-aucpr:0.79380\ttrain-aucpr:0.99998\n",
      "[789]\teval-aucpr:0.79369\ttrain-aucpr:0.99998\n",
      "[790]\teval-aucpr:0.79370\ttrain-aucpr:0.99998\n",
      "[791]\teval-aucpr:0.79362\ttrain-aucpr:0.99998\n",
      "[792]\teval-aucpr:0.79359\ttrain-aucpr:0.99998\n",
      "[793]\teval-aucpr:0.79364\ttrain-aucpr:0.99998\n",
      "[794]\teval-aucpr:0.79359\ttrain-aucpr:0.99998\n",
      "[795]\teval-aucpr:0.79356\ttrain-aucpr:0.99998\n",
      "[796]\teval-aucpr:0.79357\ttrain-aucpr:0.99998\n",
      "[797]\teval-aucpr:0.79357\ttrain-aucpr:0.99998\n",
      "[798]\teval-aucpr:0.79355\ttrain-aucpr:0.99998\n",
      "[799]\teval-aucpr:0.79358\ttrain-aucpr:0.99998\n",
      "[800]\teval-aucpr:0.79355\ttrain-aucpr:0.99998\n",
      "[801]\teval-aucpr:0.79357\ttrain-aucpr:0.99998\n",
      "[802]\teval-aucpr:0.79361\ttrain-aucpr:0.99998\n",
      "[803]\teval-aucpr:0.79361\ttrain-aucpr:0.99998\n",
      "[804]\teval-aucpr:0.79361\ttrain-aucpr:0.99998\n",
      "[805]\teval-aucpr:0.79363\ttrain-aucpr:0.99998\n",
      "[806]\teval-aucpr:0.79355\ttrain-aucpr:0.99998\n",
      "[807]\teval-aucpr:0.79361\ttrain-aucpr:0.99998\n",
      "[808]\teval-aucpr:0.79351\ttrain-aucpr:0.99998\n",
      "[809]\teval-aucpr:0.79349\ttrain-aucpr:0.99998\n",
      "[810]\teval-aucpr:0.79339\ttrain-aucpr:0.99998\n",
      "[811]\teval-aucpr:0.79337\ttrain-aucpr:0.99998\n",
      "[812]\teval-aucpr:0.79345\ttrain-aucpr:0.99999\n",
      "[813]\teval-aucpr:0.79345\ttrain-aucpr:0.99999\n",
      "[814]\teval-aucpr:0.79349\ttrain-aucpr:0.99999\n",
      "[815]\teval-aucpr:0.79355\ttrain-aucpr:0.99999\n",
      "[816]\teval-aucpr:0.79359\ttrain-aucpr:0.99999\n",
      "[817]\teval-aucpr:0.79352\ttrain-aucpr:0.99999\n",
      "[818]\teval-aucpr:0.79351\ttrain-aucpr:0.99999\n",
      "[819]\teval-aucpr:0.79361\ttrain-aucpr:0.99999\n",
      "[820]\teval-aucpr:0.79359\ttrain-aucpr:0.99999\n",
      "[821]\teval-aucpr:0.79360\ttrain-aucpr:0.99999\n",
      "[822]\teval-aucpr:0.79361\ttrain-aucpr:0.99999\n",
      "[823]\teval-aucpr:0.79359\ttrain-aucpr:0.99999\n",
      "[824]\teval-aucpr:0.79353\ttrain-aucpr:0.99999\n",
      "[825]\teval-aucpr:0.79353\ttrain-aucpr:0.99999\n",
      "[826]\teval-aucpr:0.79360\ttrain-aucpr:0.99999\n",
      "[827]\teval-aucpr:0.79352\ttrain-aucpr:0.99999\n",
      "[828]\teval-aucpr:0.79354\ttrain-aucpr:0.99999\n",
      "[829]\teval-aucpr:0.79347\ttrain-aucpr:0.99999\n",
      "[830]\teval-aucpr:0.79321\ttrain-aucpr:0.99999\n",
      "[831]\teval-aucpr:0.79327\ttrain-aucpr:0.99999\n",
      "[832]\teval-aucpr:0.79323\ttrain-aucpr:0.99999\n",
      "[833]\teval-aucpr:0.79322\ttrain-aucpr:0.99999\n",
      "[834]\teval-aucpr:0.79321\ttrain-aucpr:0.99999\n",
      "[835]\teval-aucpr:0.79316\ttrain-aucpr:0.99999\n",
      "[836]\teval-aucpr:0.79310\ttrain-aucpr:0.99999\n",
      "[837]\teval-aucpr:0.79310\ttrain-aucpr:0.99999\n",
      "[838]\teval-aucpr:0.79315\ttrain-aucpr:0.99999\n",
      "[839]\teval-aucpr:0.79325\ttrain-aucpr:0.99999\n",
      "[840]\teval-aucpr:0.79331\ttrain-aucpr:0.99999\n",
      "[841]\teval-aucpr:0.79337\ttrain-aucpr:0.99999\n",
      "[842]\teval-aucpr:0.79327\ttrain-aucpr:0.99999\n",
      "[843]\teval-aucpr:0.79310\ttrain-aucpr:0.99999\n",
      "[844]\teval-aucpr:0.79314\ttrain-aucpr:0.99999\n",
      "[845]\teval-aucpr:0.79301\ttrain-aucpr:0.99999\n",
      "[846]\teval-aucpr:0.79301\ttrain-aucpr:0.99999\n",
      "[847]\teval-aucpr:0.79295\ttrain-aucpr:0.99999\n",
      "[848]\teval-aucpr:0.79291\ttrain-aucpr:0.99999\n",
      "[849]\teval-aucpr:0.79297\ttrain-aucpr:0.99999\n",
      "[850]\teval-aucpr:0.79300\ttrain-aucpr:0.99999\n",
      "[851]\teval-aucpr:0.79282\ttrain-aucpr:0.99999\n",
      "[852]\teval-aucpr:0.79291\ttrain-aucpr:0.99999\n",
      "[853]\teval-aucpr:0.79295\ttrain-aucpr:0.99999\n",
      "[854]\teval-aucpr:0.79296\ttrain-aucpr:0.99999\n",
      "[855]\teval-aucpr:0.79296\ttrain-aucpr:0.99999\n",
      "[856]\teval-aucpr:0.79301\ttrain-aucpr:0.99999\n",
      "[857]\teval-aucpr:0.79299\ttrain-aucpr:0.99999\n",
      "[858]\teval-aucpr:0.79300\ttrain-aucpr:0.99999\n",
      "[859]\teval-aucpr:0.79294\ttrain-aucpr:0.99999\n",
      "[860]\teval-aucpr:0.79288\ttrain-aucpr:0.99999\n",
      "[861]\teval-aucpr:0.79283\ttrain-aucpr:0.99999\n",
      "[862]\teval-aucpr:0.79280\ttrain-aucpr:0.99999\n",
      "[863]\teval-aucpr:0.79273\ttrain-aucpr:0.99999\n",
      "[864]\teval-aucpr:0.79264\ttrain-aucpr:0.99999\n",
      "[865]\teval-aucpr:0.79250\ttrain-aucpr:0.99999\n",
      "[866]\teval-aucpr:0.79211\ttrain-aucpr:0.99999\n",
      "[867]\teval-aucpr:0.79214\ttrain-aucpr:0.99999\n",
      "[868]\teval-aucpr:0.79210\ttrain-aucpr:0.99999\n",
      "[869]\teval-aucpr:0.79203\ttrain-aucpr:0.99999\n",
      "[870]\teval-aucpr:0.79199\ttrain-aucpr:0.99999\n",
      "[871]\teval-aucpr:0.79191\ttrain-aucpr:0.99999\n",
      "[872]\teval-aucpr:0.79207\ttrain-aucpr:0.99999\n",
      "[873]\teval-aucpr:0.79210\ttrain-aucpr:0.99999\n",
      "[874]\teval-aucpr:0.79203\ttrain-aucpr:0.99999\n",
      "[875]\teval-aucpr:0.79211\ttrain-aucpr:0.99999\n",
      "[876]\teval-aucpr:0.79212\ttrain-aucpr:0.99999\n",
      "[877]\teval-aucpr:0.79210\ttrain-aucpr:0.99999\n",
      "[878]\teval-aucpr:0.79218\ttrain-aucpr:0.99999\n",
      "[879]\teval-aucpr:0.79203\ttrain-aucpr:1.00000\n",
      "[880]\teval-aucpr:0.79212\ttrain-aucpr:1.00000\n",
      "[881]\teval-aucpr:0.79198\ttrain-aucpr:1.00000\n",
      "[882]\teval-aucpr:0.79203\ttrain-aucpr:1.00000\n",
      "[883]\teval-aucpr:0.79198\ttrain-aucpr:1.00000\n",
      "[884]\teval-aucpr:0.79196\ttrain-aucpr:1.00000\n",
      "[885]\teval-aucpr:0.79186\ttrain-aucpr:1.00000\n",
      "[886]\teval-aucpr:0.79183\ttrain-aucpr:1.00000\n",
      "[887]\teval-aucpr:0.79181\ttrain-aucpr:1.00000\n",
      "[888]\teval-aucpr:0.79182\ttrain-aucpr:1.00000\n",
      "[889]\teval-aucpr:0.79177\ttrain-aucpr:1.00000\n",
      "[890]\teval-aucpr:0.79176\ttrain-aucpr:1.00000\n",
      "[891]\teval-aucpr:0.79181\ttrain-aucpr:1.00000\n",
      "[892]\teval-aucpr:0.79185\ttrain-aucpr:1.00000\n",
      "[893]\teval-aucpr:0.79195\ttrain-aucpr:1.00000\n",
      "[894]\teval-aucpr:0.79192\ttrain-aucpr:1.00000\n",
      "[895]\teval-aucpr:0.79187\ttrain-aucpr:1.00000\n",
      "[896]\teval-aucpr:0.79188\ttrain-aucpr:1.00000\n",
      "[897]\teval-aucpr:0.79187\ttrain-aucpr:1.00000\n",
      "[898]\teval-aucpr:0.79182\ttrain-aucpr:1.00000\n",
      "[899]\teval-aucpr:0.79171\ttrain-aucpr:1.00000\n",
      "[900]\teval-aucpr:0.79163\ttrain-aucpr:1.00000\n",
      "[901]\teval-aucpr:0.79170\ttrain-aucpr:1.00000\n",
      "[902]\teval-aucpr:0.79162\ttrain-aucpr:1.00000\n",
      "[903]\teval-aucpr:0.79149\ttrain-aucpr:1.00000\n",
      "[904]\teval-aucpr:0.79152\ttrain-aucpr:1.00000\n",
      "[905]\teval-aucpr:0.79151\ttrain-aucpr:1.00000\n",
      "[906]\teval-aucpr:0.79142\ttrain-aucpr:1.00000\n",
      "[907]\teval-aucpr:0.79139\ttrain-aucpr:1.00000\n",
      "[908]\teval-aucpr:0.79138\ttrain-aucpr:1.00000\n",
      "[909]\teval-aucpr:0.79137\ttrain-aucpr:1.00000\n",
      "[910]\teval-aucpr:0.79132\ttrain-aucpr:1.00000\n",
      "[911]\teval-aucpr:0.79134\ttrain-aucpr:1.00000\n",
      "[912]\teval-aucpr:0.79128\ttrain-aucpr:1.00000\n",
      "[913]\teval-aucpr:0.79122\ttrain-aucpr:1.00000\n",
      "[914]\teval-aucpr:0.79118\ttrain-aucpr:1.00000\n",
      "[915]\teval-aucpr:0.79121\ttrain-aucpr:1.00000\n",
      "[916]\teval-aucpr:0.79121\ttrain-aucpr:1.00000\n",
      "[917]\teval-aucpr:0.79119\ttrain-aucpr:1.00000\n",
      "[918]\teval-aucpr:0.79120\ttrain-aucpr:1.00000\n",
      "[919]\teval-aucpr:0.79119\ttrain-aucpr:1.00000\n",
      "[920]\teval-aucpr:0.79121\ttrain-aucpr:1.00000\n",
      "[921]\teval-aucpr:0.79124\ttrain-aucpr:1.00000\n",
      "[922]\teval-aucpr:0.79121\ttrain-aucpr:1.00000\n",
      "[923]\teval-aucpr:0.79121\ttrain-aucpr:1.00000\n",
      "[924]\teval-aucpr:0.79117\ttrain-aucpr:1.00000\n",
      "[925]\teval-aucpr:0.79108\ttrain-aucpr:1.00000\n",
      "[926]\teval-aucpr:0.79107\ttrain-aucpr:1.00000\n",
      "[927]\teval-aucpr:0.79107\ttrain-aucpr:1.00000\n",
      "[928]\teval-aucpr:0.79106\ttrain-aucpr:1.00000\n",
      "[929]\teval-aucpr:0.79110\ttrain-aucpr:1.00000\n",
      "[930]\teval-aucpr:0.79109\ttrain-aucpr:1.00000\n",
      "[931]\teval-aucpr:0.79094\ttrain-aucpr:1.00000\n",
      "[932]\teval-aucpr:0.79093\ttrain-aucpr:1.00000\n",
      "[933]\teval-aucpr:0.79099\ttrain-aucpr:1.00000\n",
      "[934]\teval-aucpr:0.79091\ttrain-aucpr:1.00000\n",
      "[935]\teval-aucpr:0.79095\ttrain-aucpr:1.00000\n",
      "[936]\teval-aucpr:0.79100\ttrain-aucpr:1.00000\n",
      "[937]\teval-aucpr:0.79099\ttrain-aucpr:1.00000\n",
      "[938]\teval-aucpr:0.79104\ttrain-aucpr:1.00000\n",
      "[939]\teval-aucpr:0.79104\ttrain-aucpr:1.00000\n",
      "[940]\teval-aucpr:0.79106\ttrain-aucpr:1.00000\n",
      "[941]\teval-aucpr:0.79107\ttrain-aucpr:1.00000\n",
      "[942]\teval-aucpr:0.79110\ttrain-aucpr:1.00000\n",
      "[943]\teval-aucpr:0.79104\ttrain-aucpr:1.00000\n",
      "[944]\teval-aucpr:0.79104\ttrain-aucpr:1.00000\n",
      "[945]\teval-aucpr:0.79099\ttrain-aucpr:1.00000\n",
      "[946]\teval-aucpr:0.79102\ttrain-aucpr:1.00000\n",
      "[947]\teval-aucpr:0.79100\ttrain-aucpr:1.00000\n",
      "[948]\teval-aucpr:0.79099\ttrain-aucpr:1.00000\n",
      "[949]\teval-aucpr:0.79086\ttrain-aucpr:1.00000\n",
      "[950]\teval-aucpr:0.79086\ttrain-aucpr:1.00000\n",
      "[951]\teval-aucpr:0.79074\ttrain-aucpr:1.00000\n",
      "[952]\teval-aucpr:0.79069\ttrain-aucpr:1.00000\n",
      "[953]\teval-aucpr:0.79062\ttrain-aucpr:1.00000\n",
      "[954]\teval-aucpr:0.79068\ttrain-aucpr:1.00000\n",
      "[955]\teval-aucpr:0.79062\ttrain-aucpr:1.00000\n",
      "[956]\teval-aucpr:0.79056\ttrain-aucpr:1.00000\n",
      "[957]\teval-aucpr:0.79054\ttrain-aucpr:1.00000\n",
      "[958]\teval-aucpr:0.79053\ttrain-aucpr:1.00000\n",
      "[959]\teval-aucpr:0.79049\ttrain-aucpr:1.00000\n",
      "[960]\teval-aucpr:0.79047\ttrain-aucpr:1.00000\n",
      "[961]\teval-aucpr:0.79049\ttrain-aucpr:1.00000\n",
      "[962]\teval-aucpr:0.79048\ttrain-aucpr:1.00000\n",
      "[963]\teval-aucpr:0.79045\ttrain-aucpr:1.00000\n",
      "[964]\teval-aucpr:0.79044\ttrain-aucpr:1.00000\n",
      "[965]\teval-aucpr:0.79046\ttrain-aucpr:1.00000\n",
      "[966]\teval-aucpr:0.79038\ttrain-aucpr:1.00000\n",
      "[967]\teval-aucpr:0.79033\ttrain-aucpr:1.00000\n",
      "[968]\teval-aucpr:0.79028\ttrain-aucpr:1.00000\n",
      "[969]\teval-aucpr:0.79029\ttrain-aucpr:1.00000\n",
      "[970]\teval-aucpr:0.79027\ttrain-aucpr:1.00000\n",
      "[971]\teval-aucpr:0.79026\ttrain-aucpr:1.00000\n",
      "[972]\teval-aucpr:0.79023\ttrain-aucpr:1.00000\n",
      "[973]\teval-aucpr:0.79010\ttrain-aucpr:1.00000\n",
      "[974]\teval-aucpr:0.79000\ttrain-aucpr:1.00000\n",
      "[975]\teval-aucpr:0.78992\ttrain-aucpr:1.00000\n",
      "[976]\teval-aucpr:0.78997\ttrain-aucpr:1.00000\n",
      "[977]\teval-aucpr:0.78994\ttrain-aucpr:1.00000\n",
      "[978]\teval-aucpr:0.79001\ttrain-aucpr:1.00000\n",
      "[979]\teval-aucpr:0.79002\ttrain-aucpr:1.00000\n",
      "[980]\teval-aucpr:0.79000\ttrain-aucpr:1.00000\n",
      "[981]\teval-aucpr:0.78997\ttrain-aucpr:1.00000\n",
      "[982]\teval-aucpr:0.78988\ttrain-aucpr:1.00000\n",
      "[983]\teval-aucpr:0.78983\ttrain-aucpr:1.00000\n",
      "[984]\teval-aucpr:0.78980\ttrain-aucpr:1.00000\n",
      "[985]\teval-aucpr:0.78981\ttrain-aucpr:1.00000\n",
      "[986]\teval-aucpr:0.78980\ttrain-aucpr:1.00000\n",
      "[987]\teval-aucpr:0.78973\ttrain-aucpr:1.00000\n",
      "[988]\teval-aucpr:0.78967\ttrain-aucpr:1.00000\n",
      "[989]\teval-aucpr:0.78962\ttrain-aucpr:1.00000\n",
      "[990]\teval-aucpr:0.78950\ttrain-aucpr:1.00000\n",
      "[991]\teval-aucpr:0.78960\ttrain-aucpr:1.00000\n",
      "[992]\teval-aucpr:0.78957\ttrain-aucpr:1.00000\n",
      "[993]\teval-aucpr:0.78960\ttrain-aucpr:1.00000\n",
      "[994]\teval-aucpr:0.78969\ttrain-aucpr:1.00000\n",
      "[995]\teval-aucpr:0.78976\ttrain-aucpr:1.00000\n",
      "[996]\teval-aucpr:0.78976\ttrain-aucpr:1.00000\n",
      "[997]\teval-aucpr:0.78971\ttrain-aucpr:1.00000\n",
      "[998]\teval-aucpr:0.78977\ttrain-aucpr:1.00000\n"
     ]
    }
   ],
   "source": [
    "# Retrain the model with the best hyperparameter\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eta': round(params['eta'], 1),\n",
    "    'max_depth': round(params['max_depth']),\n",
    "    'gamma': round(params['gamma']),\n",
    "    'eval_metric': 'aucpr',\n",
    "}\n",
    "\n",
    "# Create a list of xgb.DMatrix\n",
    "watch_list = [\n",
    "                (test_xgb_matrix, 'eval'),\n",
    "                (training_xgb_matrix, 'train')\n",
    "            ]\n",
    "\n",
    "# Train the model with the selected hyperparameters\n",
    "xgb_model = xgb.train(params,\n",
    "                          training_xgb_matrix,\n",
    "                          num_boost_round=999,\n",
    "                          evals=watch_list,\n",
    "                          early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1748304b-aa10-4b0b-85a3-cb4269796639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify_type(y_pred, y_label):\n",
    "    \"\"\"\n",
    "    Get classification type\n",
    "    \n",
    "    Inputs:\n",
    "    -------\n",
    "    y_pred : numpy.ndarray\n",
    "        The predictions result\n",
    "    y_label : numpy.ndarray\n",
    "        The actual label\n",
    "    Returns:\n",
    "    --------\n",
    "    rs : str\n",
    "        The clasification type\n",
    "    \"\"\"\n",
    "    \n",
    "    rs = 'TP' if y_pred == 1 and y_label == 1 else 'FP' if y_pred == 1 and y_label == 0 else 'TN' if y_pred == 0 and y_label == 0 else 'FN'\n",
    "    return rs\n",
    "\n",
    "def evaluation(threshold):\n",
    "    \"\"\"\n",
    "    Get the evaluation result\n",
    "    Inputs:\n",
    "    -------\n",
    "    threshold: float\n",
    "            The value that we need to compare the prediction result with\n",
    "    Returns:\n",
    "    --------\n",
    "    precision : float\n",
    "            The model precision\n",
    "    recall : float\n",
    "        The recall ratio\n",
    "    fscore : float\n",
    "        The F1 score\n",
    "    accuracy : float\n",
    "        The accuracy of the model\n",
    "    y_test: numpy.ndarray\n",
    "        The test label\n",
    "    y_predict : numpy.ndarray\n",
    "            The predicted label\n",
    "    \"\"\"\n",
    "    test_evaluation = test.copy()\n",
    "    predictions = xgb_model.predict(test_xgb_matrix)\n",
    "    test_evaluation['label'] = test_evaluation.apply(lambda x: 0 if x['salary'] == \"<=50k\" else 1, axis=1)\n",
    "    test_evaluation['predicted_score'] = predictions\n",
    "    test_evaluation['predicted_label'] = test_evaluation.apply(lambda x: 1 if x['predicted_score'] >= threshold else 0, axis = 1)\n",
    "    test_evaluation['type'] = test_evaluation.apply(lambda x: classify_type(x['predicted_label'], x['label']), axis = 1)\n",
    "    y_predict = test_evaluation['predicted_label'].tolist()\n",
    "    precision, recall, fscore, support = score(y_test, y_predict)\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "\n",
    "    return round(recall[1], 2), round(precision[1], 2), round(fscore[1], 2), round(accuracy, 2), y_test, y_predict, support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "716c7af9-b3ef-4608-8a0c-0db0fc785319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the evaluation result\n",
    "threshold = 0.5\n",
    "recall, precision, fscore, accuracy, y_test, y_predict, support = evaluation(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45a76753-b675-4c0a-82c6-28ad79e94702",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.64\n",
      "precision: 0.73\n",
      "fscore: 0.68\n",
      "support: [4905 1603]\n",
      "accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation result\n",
    "print('recall: {}'.format(recall))\n",
    "print('precision: {}'.format(precision))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print('accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cbd3d94-66c0-427a-9aee-b3332b1c1995",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90      4905\n",
      "           1       0.73      0.64      0.68      1603\n",
      "\n",
      "    accuracy                           0.85      6508\n",
      "   macro avg       0.81      0.78      0.79      6508\n",
      "weighted avg       0.85      0.85      0.85      6508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report result\n",
    "print(classification_report(y_test, y_predict, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada76b3-1056-4c32-bdc8-63e71e711c9a",
   "metadata": {},
   "source": [
    "We noticed that the model give better result for the label 0 than the label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61d1e163-aaf2-4593-a577-1d4c0707aa01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>person_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>4905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  salary  person_count\n",
       "0  <=50K          4905\n",
       "1   >50K          1603"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the distribution of the salary\n",
    "test.groupby(['salary']).agg(person_count=(\"salary\", \"count\")).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e53fa-a0f3-4f61-8631-6cb0b27f26bc",
   "metadata": {},
   "source": [
    "This model can be improve if we do explainable AI to find what is wrong with the data, maybe find the bias and analyse the False Positive and the False negative to know why we get thos results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17bd181-e98a-426b-8fef-30df98c73702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
